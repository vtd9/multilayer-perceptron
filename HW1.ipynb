{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "EeWhqjZ5-gcB",
        "t7v6weDTiymY",
        "uWe1jz6-4JoH",
        "I9ZZcb30LB9b",
        "gTQjTpXU-mC4",
        "aOpXBwW6FMLR"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vtd9/deep-learning/blob/explosion/HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 1\n",
        "\n",
        "CS 637, Spring 2022\n",
        "\n",
        "Due 3/3/21\n",
        "\n",
        "1) Implement an API to create a fully-connected multilayer perceptron (MLP) for a *k*-class classification problem.\n",
        "\n",
        "2) Test this implementation on MNIST data."
      ],
      "metadata": {
        "id": "GnTdZZ4lrWeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notation\n",
        "\n",
        "*  `w`: weights\n",
        "*  `b`: biases\n",
        "*  `z`: aggregated values\n",
        "*  `a`: activated values\n",
        "*  `x`: input to a neural network\n",
        "*  `yhat`: final output from the neural network (i.e., *after* softmax if applied)"
      ],
      "metadata": {
        "id": "aB0N4lx1NH_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "metadata": {
        "id": "WDcWMVP-46qe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. API"
      ],
      "metadata": {
        "id": "OHuM8L8m3E5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Layer class"
      ],
      "metadata": {
        "id": "EeWhqjZ5-gcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer(object):\n",
        "  '''\n",
        "  Represents one layer of a multilayer perceptron (MLP).\n",
        "\n",
        "  Initializes and updates parameter values, weights (w) and bias (b), for \n",
        "  aggregation within the layer.\n",
        "\n",
        "  '''\n",
        "\n",
        "  def __init__(self, in_dim=1, out_dim=1, activ_fun=None, init_with_normal=True, \n",
        "               mean=0, var=0.01, min=-0.1, max=0.1, activated=None):\n",
        "    '''\n",
        "    Constructs a new Layer object.\n",
        "\n",
        "    Args:\n",
        "     in_dim (int): Dimension of inputs to this layer\n",
        "     out_dim (int): Dimension of outputs from this layer\n",
        "     activ_fun (list or tuple): Container of functions from the Activation \n",
        "      class to apply to each layer\n",
        "     init_with_normal (bool): True to initialize parameters by sampling from a \n",
        "      normal distribution, false to sample from a uniform distribution\n",
        "     mean (float): Mean to use if initializing with a normal dist\n",
        "     var (float): Variance to use if initializing with a normal dist\n",
        "     min (float): Minimum bound to use if initializing with a uniform dist\n",
        "     max (float): Maximum bound to use if initializing with a uniform dist\n",
        "     activated (ndarray): Array to supply in special cases (i.e., input layer)\n",
        "      to explicitly set the activation values before any data is passed through\n",
        "      \n",
        "    '''\n",
        "    self.activ_fun = activ_fun\n",
        "    self.w = None # Weights\n",
        "    self.b = None # Bias\n",
        "    self.z = None # Aggregations\n",
        "    self.a = activated # Activations\n",
        "\n",
        "    # Initialize layer's parameters if a non-input layer\n",
        "    if activ_fun is not None:\n",
        "      if init_with_normal:\n",
        "        self.normal_init(out_dim, in_dim, mean, var) # Weights\n",
        "        self.normal_init(out_dim, 1, mean, var, True) # Bias\n",
        "      else:\n",
        "        self.random_init(out_dim, in_dim, min, max) # Weights\n",
        "        self.random_init(out_dim, 1, min, max, is_bias=True) # Bias\n",
        "\n",
        "  def normal_init(self, out_dim, in_dim, mean=0.0, var=0.01, is_bias=False):\n",
        "    '''\n",
        "    Initialize parameters by sampling from a normal distribution.\n",
        "\n",
        "    Args:\n",
        "\n",
        "\n",
        "    '''\n",
        "    # If variance not set, use Kaiming's initializaion with fan-in\n",
        "\n",
        "    # Initialize weights or bias\n",
        "    if is_bias:\n",
        "      self.b = np.random.normal(mean, var**0.5, (out_dim, in_dim))\n",
        "    else:\n",
        "      self.w = np.random.normal(mean, var**0.5, (out_dim, in_dim))\n",
        "\n",
        "  def random_init(self, out_dim, in_dim, min=-0.1, max=0.1, is_bias=False):\n",
        "    '''\n",
        "    Initialize parameters by sampling from a uniform distribution.\n",
        "\n",
        "    Args:\n",
        "\n",
        "\n",
        "    '''\n",
        "    if is_bias:\n",
        "      self.b = np.random.uniform(min, max, size=(out_dim, in_dim))\n",
        "    else:\n",
        "      self.w = np.random.uniform(min, max, size=(out_dim, in_dim))\n",
        "\n",
        "  def zero_bias(self):\n",
        "    '''\n",
        "    Set layer biases to zero.\n",
        "\n",
        "    '''\n",
        "    self.b = np.zeros(self.b.shape)\n",
        "  \n",
        "  def forward(self, input, batch_size):\n",
        "    '''\n",
        "    Pass inputs forward through the layer.\n",
        "    \n",
        "    Args:\n",
        "      input (ndarray): Feature values to pass through the model\n",
        "      batch_size (int): Batch size of the input\n",
        "\n",
        "    '''\n",
        "    # Aggregate\n",
        "    self.z = self.w @ input + self.b\n",
        "    if np.isinf(self.z).any(): \n",
        "      print(self.z, self.w, self.b)\n",
        "      raise ArithmeticError('Possibly exploding gradient')\n",
        "\n",
        "    # Activate\n",
        "    self.a = (self.activ_fun(self.z)).reshape(-1, batch_size)\n",
        "  \n",
        "  def adjust_weight(self, prev_a, grad_chain, lr, batch_size):\n",
        "    '''\n",
        "    Update weight values using the backpropagated gradient.\n",
        "    \n",
        "    Args:\n",
        "      prev_a (ndarray): Activation from the previous layer\n",
        "      grad_chain (ndarray): Backpropagated chain of derivatives so far\n",
        "      lr: Learning rate\n",
        "\n",
        "    '''\n",
        "    # Normalize magnitude of adjustment with batch size\n",
        "    self.w -= lr/batch_size * (grad_chain @ prev_a.T)\n",
        "  \n",
        "  def adjust_bias(self, grad_chain, lr, batch_size):\n",
        "    '''\n",
        "    Update bias values using the backpropagated gradient.\n",
        "\n",
        "    Args:\n",
        "      grad_chain (ndarray): Backpropagated chain of derivatives so far\n",
        "      lr (float): Learning rate\n",
        "      batch_size (int): Batch size\n",
        "\n",
        "    '''\n",
        "    # Derivative of an aggregate wrt bias (dz/db) = 1, so don't need the \n",
        "    # activation values of the previous layer\n",
        "    self.b -= lr/batch_size * np.average(grad_chain, axis=1).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "TXeX29VN5ZYx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Perceptron class"
      ],
      "metadata": {
        "id": "z-GYqijcKkcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron(object):\n",
        "  '''\n",
        "  Represents a multi-layer perceptron (MLP) for processing input data. \n",
        "  \n",
        "  Made of an aggregate of Layer objects that can be accessed from indexes,\n",
        "  where 0 is the input layer, 1 is the first hidden layer, and so forth.\n",
        "\n",
        "  '''\n",
        "\n",
        "  def __init__(self, dims, activ_fns, loss_fn,\n",
        "               init_with_normal=True, mean=0.0, var=0.01, min=-0.1, max=0.1):\n",
        "    '''\n",
        "    Constructs a new Perceptron object.\n",
        "\n",
        "    Args:\n",
        "      dims (list or tuple): Container of ints, each representing the \n",
        "        dimensionality of a layer at the same index. First integer should be\n",
        "        the number of input features; last int is the number of output neurons.\n",
        "      activ_fns (list or tuple): Container of methods from the Activation class \n",
        "      to apply on the hidden and output aggregations\n",
        "      loss_fn: loss function to measure model performance\n",
        "      init_with_normal (bool): True to initialize parameters by sampling from a \n",
        "        normal distribution, false to sample from a uniform one\n",
        "      mean (float): Mean to use if initializing with a normal dist\n",
        "      var (float): Variance to use if initializing with a normal dist\n",
        "      min (float): Minimum bound to use if initializing with a uniform dist\n",
        "      max (float): Minimum bound to use if initializing with a uniform dist\n",
        "\n",
        "    '''\n",
        "    # Ensure Perceptron has at least input and output layers\n",
        "    assert len(dims) > 1\n",
        "\n",
        "    # Ensure number of activation functions matches number of layers - 1\n",
        "    assert len(dims) == len(activ_fns) + 1\n",
        "    \n",
        "    self.dims = dims\n",
        "    self.n_layers = len(dims)\n",
        "    self.activ_fns = activ_fns\n",
        "    self.loss_fn = loss_fn\n",
        "    self.reset(init_with_normal, mean, var, min, max) # Initialize parameters\n",
        "\n",
        "  def reset(self, init_with_normal=True, mean=0.0, var=0.01, min=-0.1, max=0.1):  \n",
        "    '''\n",
        "    Initialize parameters in the MLP.\n",
        "\n",
        "    Args:\n",
        "\n",
        "\n",
        "    '''\n",
        "    self.layers = [None] * len(self.dims)\n",
        "\n",
        "    # For each layer besides the input layer, attach the appropriate dimensions, \n",
        "    # activation function, and initialization parameters\n",
        "    for i in range(self.n_layers - 1):\n",
        "      self.layers[i + 1] = Layer(\n",
        "          in_dim=self.dims[i], out_dim=self.dims[i+1],\n",
        "          activ_fun=self.activ_fns[i], \n",
        "          init_with_normal=init_with_normal,\n",
        "           mean=mean, var=var, min=min, max=max)\n",
        "  \n",
        "  def zero_biases(self):\n",
        "    '''\n",
        "    Set all biases in the MLP to zero.\n",
        "\n",
        "    '''\n",
        "    for i in range(1, self.n_layers):\n",
        "      self.layers[i].zero_bias()\n",
        "\n",
        "  def __getitem__(self, layer_index):\n",
        "    '''\n",
        "    Retrieves reference to a layer in the MLP.\n",
        "\n",
        "    Args:\n",
        "      layer_index (int): Index of layer to access.\n",
        "\n",
        "    Returns: \n",
        "      Layer object at layer_index\n",
        "\n",
        "    '''\n",
        "    return self.layers[layer_index]\n",
        "  \n",
        "  def forward(self, X, batch_size):\n",
        "    '''\n",
        "    Step forward through the MLP, aggregating the previous layer's values and\n",
        "    then applying the chosen activation function.\n",
        "\n",
        "    Args:\n",
        "      X (ndarray): Input data to feed through the network\n",
        "      batch_size (int): Batch size\n",
        "\n",
        "    '''\n",
        "    # Set the first layer (the input layer)'s \"activation\" to the input data\n",
        "    self.layers[0] = Layer(activated=X)\n",
        "\n",
        "    # Pass through each layer using inputs from previous layer\n",
        "    for i, layer in enumerate(self.layers[1:], start=1):\n",
        "      layer.forward(self.layers[i-1].a, batch_size)\n",
        "  \n",
        "  def backward(self, labels, lr, batch_size):\n",
        "    '''\n",
        "    Backpropagates from loss and adjust parameters after a forward pass.\n",
        "    \n",
        "    Args:\n",
        "      labels (ndarray): True labels in a one-hot representation\n",
        "      lr (float): Learning rate\n",
        "      batch_size (int): Batch size\n",
        "    \n",
        "    '''\n",
        "    # Calculate derivative of loss wrt logits\n",
        "    grad_chain = self.loss_fn(self.layers[-1].a, labels, derive=True)\n",
        "\n",
        "    for i in range(self.n_layers - 1, 0, -1):\n",
        "      # Save current weight for dloss/da of the previous layer later\n",
        "      prev_a = self.layers[i-1].a\n",
        "      old_weight = self.layers[i].w\n",
        "\n",
        "      # Adjust parameters\n",
        "      self.layers[i].adjust_weight(prev_a, grad_chain, lr, batch_size)\n",
        "      self.layers[i].adjust_bias(grad_chain, lr, batch_size)\n",
        "\n",
        "      # Compute dloss/da of previous layer, multiply to current product\n",
        "      grad_chain = (old_weight.T @ grad_chain) \n",
        "\n",
        "      # Compute dloss/dz of previous layer using elementwise multiplication\n",
        "      if self.layers[i-1].activ_fun is not None:\n",
        "        da_dz = self.layers[i-1].activ_fun(prev_a, derive=True)\n",
        "      else: # Don't need to derive the input layer\n",
        "        break\n",
        "  \n",
        "  def train(self, generator, lr, batch_size, train_mode=True):\n",
        "    '''\n",
        "    Train for a single epoch.\n",
        "\n",
        "    Args:\n",
        "      generator (generator): Generator to loop through one batch at a time\n",
        "      lr (float): Learning rate\n",
        "      train_mode (bool): True to backpropagate and update parameters\n",
        "    \n",
        "    Returns:\n",
        "      Average loss (float), average accuracy (float)\n",
        "\n",
        "    '''\n",
        "    loss = 0.0\n",
        "    acc = 0.0\n",
        "    for i, (X, y) in enumerate(generator):\n",
        "      self.forward(X, batch_size)\n",
        "\n",
        "      if train_mode: # Adjust parameters only during training\n",
        "        self.backward(y, lr, batch_size)\n",
        "\n",
        "      # Update accumulated measures\n",
        "      loss += self.loss_fn(self[-1].a, y)\n",
        "      acc += Loss.accuracy(self[-1].a, y)\n",
        "\n",
        "    # Average over the number of batches done\n",
        "    return loss/(i+1), acc/(i+1)"
      ],
      "metadata": {
        "id": "OCaLYwcnou37"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Activation and loss functions"
      ],
      "metadata": {
        "id": "t7v6weDTiymY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation(object):\n",
        "  '''\n",
        "  Common activation functions used in a neural network.\n",
        "\n",
        "  '''\n",
        "  @staticmethod\n",
        "  def relu(x, derive=False):\n",
        "    '''\n",
        "    Applies ReLU or its derivative on an input.\n",
        "    \n",
        "    Args:\n",
        "      x (ndarray): Input data to apply ReLU on\n",
        "      derive (bool): True to take derivative of ReLU wrt to X\n",
        "\n",
        "    Returns:\n",
        "      Activated or gradient value from ReLU.\n",
        "\n",
        "    '''\n",
        "    if not derive:\n",
        "      return np.maximum(x, 0., x)\n",
        "    else:\n",
        "      return (x > 0).astype(int)\n",
        "  \n",
        "  @staticmethod\n",
        "  def sigmoid(x, derive=False):\n",
        "    '''\n",
        "    Applies sigmoid or its deriative on an input.\n",
        "\n",
        "    Args:\n",
        "      x (ndarray): Input data to apply sigmoid on\n",
        "       derive (bool): True to take derivative of sigmoid wrt to X\n",
        "\n",
        "    Returns:\n",
        "      Activated or gradient value from sigmoid.\n",
        "\n",
        "    '''\n",
        "    if not derive:\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "    else: \n",
        "      return Activation.sigmoid(x) * (1-Activation.sigmoid(x))\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def softmax(x):\n",
        "    '''\n",
        "    Args:\n",
        "      x (ndarray): Input data to apply softmax on\n",
        "\n",
        "    Returns: \n",
        "      Probability values for a k-class classification so the sum of a \n",
        "      given example over all classes = 1.0\n",
        "      \n",
        "    '''\n",
        "    # Update from using a naive implementation - prevent numerical overflows\n",
        "    # by using shifting trick as opposed to np.exp(x)/np.sum(np.exp(x), axis=0)\n",
        "    shift = np.max(x)\n",
        "    exp_x = np.exp(x - shift)\n",
        "    return exp_x/np.sum(exp_x, axis=0)"
      ],
      "metadata": {
        "id": "5Ih0IHs_ixjA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss(object):\n",
        "  '''\n",
        "  Common loss functions used in a neural network.\n",
        "\n",
        "  '''\n",
        "\n",
        "  @staticmethod\n",
        "  def cross_entropy(yhat, y, epsilon=1e-5, derive=False):\n",
        "    '''\n",
        "    Computes the cross-entropy loss or its gradient with respect to logits for \n",
        "    a batch of predictions.\n",
        "\n",
        "    Args:\n",
        "      yhat (ndarray): Outputs (after activation) from a MLP\n",
        "      y (ndarray): Actual labels to compare output with\n",
        "      epsilon (float): Small positive value to prevent taking log of 0\n",
        "      derive (bool): True to return derivative wrt logits\n",
        "\n",
        "    Returns:\n",
        "      Cross-entropy loss value or its derivative wrt logits\n",
        "\n",
        "    '''\n",
        "    if not derive:\n",
        "      # Squash for each example by summing before averaging - \n",
        "      # erroneously many zeros otherwise\n",
        "      return np.average(np.sum(-y * np.log(yhat + epsilon), axis=0))\n",
        "    else:\n",
        "      return yhat - y\n",
        "\n",
        "  @staticmethod\n",
        "  def hinge_loss(yhat, y, margin=1.0, derive=False):\n",
        "    '''\n",
        "    Computes the hinge loss or its derivative for a batch of predictions.\n",
        "\n",
        "    Args:\n",
        "      yhat (ndarray): Outputs (after activation) from a MLP\n",
        "      y (ndarray): Actual labels to compare output with\n",
        "      margin (float): Margin for target class value to overtake\n",
        "      derive (bool): True to return derivative\n",
        "\n",
        "    Returns:\n",
        "      Hinge loss value or its derivative\n",
        "\n",
        "    '''\n",
        "    # Output of the target class for each example\n",
        "    # Repeat so same value going down row to get same shape as yhat\n",
        "    y_target = np.sum(np.where(y == 1, yhat, 0), axis=0)\n",
        "\n",
        "    # Get distances from outputs at indexes != target class\n",
        "    # Add margin to elements at indexes != target class\n",
        "    dist = np.where(y == 1, 0, yhat - y_target + margin)\n",
        "\n",
        "    if not derive:\n",
        "      # Get the positive differences over each example (zero-threshold)\n",
        "      pos_per_example = np.where(dist > 0, dist, 0)\n",
        "\n",
        "      # Sum over each example, average over all batches\n",
        "      return np.sum(np.sum(pos_per_example, axis=0))\n",
        "    else:\n",
        "      return\n",
        "  \n",
        "  @staticmethod\n",
        "  def accuracy(yhat, y, y_one_hot=True):\n",
        "    '''\n",
        "    Computes the accuracy for a batch of outputs.\n",
        "\n",
        "    Args:\n",
        "      yhat (ndarray): Outputs (after activation) from a MLP\n",
        "      y (ndarray): Actual labels to compare output with\n",
        "      y_one_hot: True if y given in one-hot form, false if given as a list \n",
        "        of correct indexes\n",
        "\n",
        "    Returns:\n",
        "      Accuracy of a given output against its true values\n",
        "\n",
        "    '''\n",
        "    # Prediction class is the one with max predicted probability in each example\n",
        "    yhat_choices = np.argmax(yhat, axis=0)\n",
        "\n",
        "    # Reverse the actual labels' representation as one-hot for easier comparison\n",
        "    if y_one_hot:\n",
        "      y = Utility.reverse_one_hot(y)\n",
        "\n",
        "    # Compute the average accuracy of the batch\n",
        "    return np.average(yhat_choices == y)"
      ],
      "metadata": {
        "id": "cKXFSrnTBkys"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMP: hinge loss bumbling\n",
        "yhat = np.random.normal(size=(4,3)).T\n",
        "y = np.zeros((4, 3)).T\n",
        "y[0, 0] = 1\n",
        "y[2, 1] = 1\n",
        "y[1, 2] = 1\n",
        "y[0, 3] = 1\n",
        "yhat, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQyT3KsAJxc3",
        "outputId": "2112133c-7210-42e1-b02f-ba524ef098dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 1.58109676, -0.82070111,  1.35998506,  1.02665171],\n",
              "        [-1.02326697,  1.56572325, -0.55458617, -1.70785564],\n",
              "        [ 0.60870623, -0.53913148,  0.39781522, -1.54012685]]),\n",
              " array([[1., 0., 0., 1.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 1., 0., 0.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMP\n",
        "y_target = np.sum(np.where(y == 1, yhat, 0), axis=0)\n",
        "dist = np.where(y == 1, 0, yhat - y_target + 1.)\n",
        "maybe_deriv = np.where(y == 1, np.sum(dist > 0, axis=0), dist > 0)\n",
        "dist, maybe_deriv"
      ],
      "metadata": {
        "id": "iWaqnbauNR8q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35cc5646-988a-4643-d964-d57dbc4f05dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0.        ,  0.71843037,  2.91457122,  0.        ],\n",
              "        [-1.60436374,  3.10485473,  0.        , -1.73450735],\n",
              "        [ 0.02760947,  0.        ,  1.95240139, -1.56677856]]),\n",
              " array([[1, 1, 1, 0],\n",
              "        [0, 1, 2, 0],\n",
              "        [1, 2, 1, 0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Utilities class"
      ],
      "metadata": {
        "id": "uWe1jz6-4JoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Utility(object):\n",
        "  '''\n",
        "  Miscellaneous methods for training and plotting.\n",
        "\n",
        "  '''\n",
        "\n",
        "  @staticmethod\n",
        "  def train_and_validate(mlp, dataset, lr, epochs, batch_size,\n",
        "                         shuffle_every_epoch=True, verbose=True):\n",
        "    '''\n",
        "    Train and validate in parallel over one or more epochs.\n",
        "\n",
        "    Args:\n",
        "      mlp: Perceptron object to train\n",
        "      dataset: Dataset object to pull features and labels from\n",
        "      lr (float): Learning rate\n",
        "      epochs (int): Number of epochs to train in\n",
        "      batch_size (int): Batch size\n",
        "      shuffle_every_epoch (bool): True to reshuffle training and validation\n",
        "        data within their divisions when regenerating the batches\n",
        "      verbose (bool): True to print loss and accuracy values after each epoch\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    \n",
        "    '''\n",
        "    assert (epochs > 0)\n",
        "    train_loss = np.empty(epochs)\n",
        "    train_acc = np.empty(epochs)\n",
        "    valid_loss = np.empty(epochs)\n",
        "    valid_acc = np.empty(epochs)\n",
        "    start_time = time.time()\n",
        "    if verbose:\n",
        "      print('Epoch \\tTrain_loss \\tTrain_acc \\tValid_loss \\tValid_acc')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      # Make iterators\n",
        "      train_iter = dataset.make_batches(batch_size, group='train', \n",
        "                                        shuffle_again=shuffle_every_epoch)\n",
        "      valid_iter = dataset.make_batches(batch_size, group='valid',\n",
        "                                        shuffle_again=shuffle_every_epoch)\n",
        "\n",
        "      # Train and validate - validate \"previous\" model before updating\n",
        "      valid_loss[epoch], valid_acc[epoch] = mlp.train(valid_iter, lr=lr,\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      train_mode=False)\n",
        "      train_loss[epoch], train_acc[epoch] = mlp.train(train_iter, lr=lr,\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      train_mode=True)\n",
        "      \n",
        "      # If set to print out info as executing\n",
        "      if verbose:\n",
        "        print('{} \\t{:9.3f} \\t{:9.3f} \\t {:9.3f} \\t{:9.3f}'.format(\n",
        "            epoch, train_loss[epoch], train_acc[epoch], \n",
        "            valid_loss[epoch], valid_acc[epoch]))\n",
        "\n",
        "    # Print amount of time taken\n",
        "    print('Time elapsed (s): {:2.1f}\\n'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "    return train_loss, train_acc, valid_loss, valid_acc\n",
        "  \n",
        "  @staticmethod\n",
        "  def make_one_hot(y):\n",
        "    '''\n",
        "    Make a traditional container of labels into a one-hot representation.\n",
        "    \n",
        "    Args:\n",
        "      y (list or tuple): Container of labels to convert\n",
        "\n",
        "    Returns:\n",
        "      Transformed array in one-hot representation\n",
        "    \n",
        "    '''\n",
        "    transformed = np.zeros((y.size, y.max() + 1))\n",
        "    transformed[np.arange(y.size), y] = 1\n",
        "    return transformed.T\n",
        "\n",
        "  @staticmethod\n",
        "  def reverse_one_hot(y):\n",
        "    '''\n",
        "    Convert a one-hot array back to a simple list of indices.\n",
        "\n",
        "    Args:\n",
        "      y (ndarray): Array to convert\n",
        "    \n",
        "    Returns:\n",
        "      Transformed array as a sequence of indexes\n",
        "\n",
        "    '''\n",
        "    return np.argmax(y, axis=0)\n",
        "\n",
        "  @staticmethod\n",
        "  def plot_images(dataset, y, num_images=3):\n",
        "    '''\n",
        "    Plot using the flattened representation of a square image in input X.\n",
        "\n",
        "    Args:\n",
        "\n",
        "    \n",
        "    '''\n",
        "    # Get random sample of dataset\n",
        "    indexes = np.random.randint(0, dataset.X.shape[-1], num_images)\n",
        "\n",
        "    # Display data as images\n",
        "    width = int(dataset.X.shape[0]**0.5)\n",
        "\n",
        "    for i in indexes:  \n",
        "      plt.subplot(330 + 1 + i)\n",
        "      plt.axis('off')\n",
        "      plt.imshow(dataset.X[:, i+i].reshape(width, width), \n",
        "                 cmap=plt.get_cmap('gray'))\n",
        "      \n",
        "      # Title each image with its true label\n",
        "      plt.title([Utility.reverse_one_hot(y[:, i+i])])\n",
        "\n",
        "      # TItle ach image with its predicted label, if any\n",
        "  \n",
        "  @staticmethod\n",
        "  def plot_results(results, labels=None, fmts=None, xlabel='Epoch', ylabel='',\n",
        "                   ymax=None, title=''):\n",
        "    \n",
        "    '''\n",
        "    Plot one or more results\n",
        "\n",
        "    Args:\n",
        "      results (list or tuple): Container of ndarray results to plot\n",
        "      labels (list or tuple): Container of strings, each a label corresponding\n",
        "        to each result to be displayed in the legend\n",
        "      fmts (list or tuple): Container of strings matching the matplotlib's\n",
        "        line, color, and marker styles. None to use default\n",
        "      xlabel (str): Label on x-axis\n",
        "      ylabel (str): Label on y-axis\n",
        "      ymax (float): Maximum bound to set y-axis to. None to use automatic\n",
        "        settings\n",
        "      title (str): Title of plot\n",
        "\n",
        "    '''\n",
        "    # Account for arguments not given\n",
        "    if labels is None: \n",
        "      labels = (str(_) for _ in range(len(results)))\n",
        "    else:\n",
        "      assert len(results) == len(labels)\n",
        "\n",
        "    if fmts is None:\n",
        "      fmts = ('' for _ in range(len(results)))\n",
        "    else:\n",
        "      assert len(results) == len(fmts)\n",
        "\n",
        "    # Plot each set of results with its corresponding label and format\n",
        "    for result, label, fmt in zip(results, labels, fmts):\n",
        "      plt.plot(np.arange(result.shape[0]), result, fmt, label=label)\n",
        "\n",
        "    # Parameters affecting entire plot\n",
        "    plt.legend()\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)  \n",
        "    if ymax is not None:\n",
        "      plt.ylim(0, ymax)"
      ],
      "metadata": {
        "id": "QMGZb_cJCAL7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Dataset class"
      ],
      "metadata": {
        "id": "I9ZZcb30LB9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(object):\n",
        "  '''\n",
        "  Represents a set of data with methods to reshape, split, and train.\n",
        "\n",
        "  '''\n",
        "  def __init__(self, X, y, make_y_one_hot=True):\n",
        "    '''\n",
        "    Constructs a Dataset object.\n",
        "    \n",
        "    Args:\n",
        "      X (ndarray): Input data\n",
        "      y (ndarray): Labels corresponding to input data\n",
        "      make_y_one_hot (bool): True to convert the labels into a one-hot \n",
        "        representation, if not done already; false to skip this step\n",
        "\n",
        "    '''\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.X_train = None\n",
        "    self.y_train = None\n",
        "    self.X_valid = None\n",
        "    self.y_valid = None\n",
        "    self.X_test = None\n",
        "    self.y_test = None\n",
        "\n",
        "    if make_y_one_hot:\n",
        "      self.y = Utility.make_one_hot(self.y)\n",
        "\n",
        "  def shape(self, features, categories, flatten_X=True):\n",
        "    '''\n",
        "    Reshapes the input and output data.\n",
        "\n",
        "    Args:\n",
        "      features: Number of features in the input data. Set to width of a 2D image\n",
        "        if it will be flattened.\n",
        "      categories: Number of categories in the classification problem\n",
        "      flatten_X: True to flatten a 2D image into a 1D array\n",
        "\n",
        "    '''\n",
        "    if flatten_X: \n",
        "      self.X = self.X.reshape(-1, features*features).T\n",
        "    else:\n",
        "      self.X = self.X.reshape(features, -1)\n",
        "    self.y = self.y.reshape(categories, -1)\n",
        "\n",
        "  def shuffle(self):\n",
        "    '''\n",
        "    Shuffles both X and y together in the dataset.\n",
        "\n",
        "    '''\n",
        "    assert self.X.shape[-1] == self.y.shape[-1]\n",
        "    shuffled_indexes = np.random.permutation(self.X.shape[-1])\n",
        "    self.X = self.X[:, shuffled_indexes]\n",
        "    self.y = self.y[:, shuffled_indexes]\n",
        "\n",
        "  def divide(self, p_train=70, p_valid=15, p_test=15):\n",
        "    '''\n",
        "    Divide the loaded data into sets for training, validation, and testing.\n",
        "\n",
        "    Args:\n",
        "      p_train (int): Percentage of data to allot for training\n",
        "      p_valid (int): Percentage of data to allot for validation\n",
        "      p_test (int): Percentage of data to allot for testing\n",
        "\n",
        "    '''\n",
        "    if (p_train + p_valid + p_test != 100):\n",
        "      raise ValueError('Error: percentages don''t sum up to 100!')\n",
        "    n_train = int(p_train * 0.01 * self.X.shape[-1])\n",
        "    n_valid = int(p_valid * 0.01 * self.X.shape[-1])\n",
        "    n_test = int(p_test * 0.01 * self.X.shape[-1])\n",
        "\n",
        "    # If missing a few examples from cutoffs, add to training set\n",
        "    test_diff = self.X.shape[-1] - (n_train + n_valid + n_test)\n",
        "    if test_diff > 0:\n",
        "      n_train += test_diff\n",
        "\n",
        "    # Define groups\n",
        "    self.X_train = self.X[:, 0:n_train]\n",
        "    self.y_train = self.y[:, 0:n_train]\n",
        "    self.X_valid = self.X[:, n_train:(n_train + n_valid)]\n",
        "    self.y_valid = self.y[:, n_train:(n_train + n_valid)]\n",
        "    self.X_test = self.X[:, (n_train + n_valid): ]\n",
        "    self.y_test = self.y[:, (n_train + n_valid): ]\n",
        "    \n",
        "  def make_batches(self, batch_size, group='train', scale_X=True,\n",
        "                   shuffle_again=True):\n",
        "    '''\n",
        "    Make batches given a specified size and a selected group.\n",
        "\n",
        "    Args: \n",
        "      batch_size (int): Batch size to divide data into\n",
        "      group (str): Either train, valid, or test to select group\n",
        "      scale_X (bool): True to rescale data to be between 0 and 1\n",
        "      shuffle_again (bool): True to shuffle data within its group when \n",
        "        a new set of batches is made\n",
        "\n",
        "    Returns:\n",
        "      Generator of batches in the specified size from the specified group.\n",
        "\n",
        "    '''\n",
        "    # Get the training, testing, or validation group:\n",
        "    if group == 'train':\n",
        "      X_select, y_select = self.X_train, self.y_train\n",
        "    elif group == 'valid':\n",
        "      X_select, y_select = self.X_valid, self.y_valid\n",
        "    elif group == 'test':\n",
        "      X_select, y_select = self.X_test, self.y_test\n",
        "    else:\n",
        "      raise ValueError('Incorrect argument for group! Choose between train, '\n",
        "                       'validate, or test.')\n",
        "\n",
        "    # Shuffle within the group, if specified\n",
        "    if shuffle_again:\n",
        "      shuffled_indexes = np.random.permutation(X_select.shape[-1])\n",
        "      X_select = X_select[:, shuffled_indexes]\n",
        "      y_select = y_select[:, shuffled_indexes]\n",
        "\n",
        "    # Make generators for the features and labels of the selected set\n",
        "    for i in range(0, X_select.shape[-1], batch_size):\n",
        "      if scale_X: # Rescale X to have values between 0 and 1\n",
        "        X_scaled = np.linalg.norm(X_select[:, i:i+batch_size])\n",
        "        yield X_select[:, i:i+batch_size]/X_scaled, y_select[:, i:i+batch_size]\n",
        "      else:\n",
        "        yield X_select[:, i:i+batch_size], y_select[:, i:i+batch_size]"
      ],
      "metadata": {
        "id": "hp-u4Odi4MHf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Testing the API"
      ],
      "metadata": {
        "id": "ejKqz_Tv2A3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Import the MNIST dataset\n",
        "*   Permitted to use outside packages for downloading MNIST data, so use `keras`.\n",
        "*   Merge existing training and test sets and resplit, since splitting must be done by this homework's API.\n",
        "\n"
      ],
      "metadata": {
        "id": "gTQjTpXU-mC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
      ],
      "metadata": {
        "id": "UFHR62sWB4eT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "880326f1-bd13-4f76-b446-5c12f6c619bb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_all_X = np.concatenate((train_X, test_X), axis=0)\n",
        "mnist_all_y = np.concatenate((train_y, test_y), axis=0)\n",
        "print('MNIST features reshaped:', mnist_all_X.shape, \n",
        "      '\\nMNIST labels reshaped:', mnist_all_y.shape)"
      ],
      "metadata": {
        "id": "jqrK7cd-E0BJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba15c54-0fb2-4bee-bd09-4ff2d31a3957"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST features reshaped: (70000, 28, 28) \n",
            "MNIST labels reshaped: (70000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Preprocessing"
      ],
      "metadata": {
        "id": "aOpXBwW6FMLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a Dataset object\n",
        "mnist_set = Dataset(mnist_all_X, mnist_all_y, True)\n",
        "\n",
        "# Reshape the data\n",
        "width = 28\n",
        "categories = 10\n",
        "mnist_set.shape(width, categories, True)\n",
        "print(\"Feature and label shapes:\", mnist_set.X.shape, mnist_set.y.shape)\n",
        "\n",
        "# Shuffle the data\n",
        "# plot first __ before shuffling\n",
        "mnist_set.shuffle()\n",
        "# plot first __ after to prove it's shuffled?\n",
        "\n",
        "# Divide the data\n",
        "mnist_set.divide()\n",
        "print('Splits on training, validation, & testing:', \n",
        "      mnist_set.X_train.shape, mnist_set.X_valid.shape, mnist_set.X_test.shape)"
      ],
      "metadata": {
        "id": "cueDPZa2qKnz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fad1925-d5cb-406e-edef-34815a9fbc2d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature and label shapes: (784, 70000) (10, 70000)\n",
            "Splits on training, validation, & testing: (784, 49000) (784, 10500) (784, 10500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and validation"
      ],
      "metadata": {
        "id": "pH2B2SnNKrVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varying the learning rate"
      ],
      "metadata": {
        "id": "D6yuzi4ey9Cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a generic neural network with two hidden layers\n",
        "p = Perceptron((width*width, 64, 32, 10),\n",
        "               (Activation.relu, Activation.relu, Activation.softmax),\n",
        "               Loss.cross_entropy, var=0.01)\n",
        "lrs = (0.2, 0.1, 0.05, 0.01) # Different learning rates to test\n",
        "epochs = 15\n",
        "lr_results = []\n",
        "for lr in lrs:\n",
        "  lr_results.append(Utility.train_and_validate(p, mnist_set, lr, epochs, 100))\n",
        "  p.reset()"
      ],
      "metadata": {
        "id": "kEKW_IuKy_an",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e615ec9-a5d7-4d8b-94da-d954615f98d8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch \tTrain_loss \tTrain_acc \tValid_loss \tValid_acc\n",
            "0 \t    2.303 \t    0.106 \t     2.307 \t    0.107\n",
            "1 \t    2.298 \t    0.113 \t     2.299 \t    0.111\n",
            "2 \t    2.283 \t    0.131 \t     2.294 \t    0.111\n",
            "3 \t    2.115 \t    0.218 \t     2.259 \t    0.194\n",
            "4 \t    1.640 \t    0.435 \t     1.883 \t    0.324\n",
            "5 \t    1.207 \t    0.580 \t     1.387 \t    0.524\n",
            "6 \t    0.912 \t    0.706 \t     1.075 \t    0.648\n",
            "7 \t    0.746 \t    0.759 \t     0.819 \t    0.733\n",
            "8 \t    0.684 \t    0.783 \t     0.727 \t    0.765\n",
            "9 \t    0.641 \t    0.801 \t     0.685 \t    0.778\n",
            "10 \t    0.600 \t    0.819 \t     0.639 \t    0.799\n",
            "11 \t    0.559 \t    0.833 \t     0.599 \t    0.814\n",
            "12 \t    0.525 \t    0.845 \t     0.553 \t    0.829\n",
            "13 \t    0.498 \t    0.855 \t     0.530 \t    0.841\n",
            "14 \t    0.473 \t    0.863 \t     0.498 \t    0.855\n",
            "Time elapsed (s): 28.3\n",
            "Epoch \tTrain_loss \tTrain_acc \tValid_loss \tValid_acc\n",
            "0 \t    2.304 \t    0.105 \t     2.313 \t    0.090\n",
            "1 \t    2.299 \t    0.113 \t     2.300 \t    0.111\n",
            "2 \t    2.297 \t    0.113 \t     2.298 \t    0.111\n",
            "3 \t    2.293 \t    0.114 \t     2.296 \t    0.111\n",
            "4 \t    2.284 \t    0.128 \t     2.290 \t    0.112\n",
            "5 \t    2.250 \t    0.196 \t     2.274 \t    0.159\n",
            "6 \t    2.105 \t    0.262 \t     2.209 \t    0.210\n",
            "7 \t    1.865 \t    0.328 \t     1.982 \t    0.272\n",
            "8 \t    1.671 \t    0.431 \t     1.772 \t    0.381\n",
            "9 \t    1.425 \t    0.536 \t     1.575 \t    0.483\n",
            "10 \t    1.171 \t    0.616 \t     1.301 \t    0.573\n",
            "11 \t    0.991 \t    0.672 \t     1.086 \t    0.634\n",
            "12 \t    0.878 \t    0.714 \t     0.945 \t    0.690\n",
            "13 \t    0.807 \t    0.741 \t     0.854 \t    0.720\n",
            "14 \t    0.760 \t    0.760 \t     0.800 \t    0.745\n",
            "Time elapsed (s): 25.4\n",
            "Epoch \tTrain_loss \tTrain_acc \tValid_loss \tValid_acc\n",
            "0 \t    2.306 \t    0.098 \t     2.313 \t    0.100\n",
            "1 \t    2.302 \t    0.110 \t     2.303 \t    0.100\n",
            "2 \t    2.300 \t    0.113 \t     2.301 \t    0.111\n",
            "3 \t    2.300 \t    0.113 \t     2.300 \t    0.111\n",
            "4 \t    2.299 \t    0.113 \t     2.299 \t    0.111\n",
            "5 \t    2.298 \t    0.113 \t     2.299 \t    0.111\n",
            "6 \t    2.297 \t    0.113 \t     2.297 \t    0.111\n",
            "7 \t    2.295 \t    0.113 \t     2.296 \t    0.111\n",
            "8 \t    2.293 \t    0.113 \t     2.294 \t    0.111\n",
            "9 \t    2.290 \t    0.113 \t     2.292 \t    0.111\n",
            "10 \t    2.286 \t    0.113 \t     2.288 \t    0.111\n",
            "11 \t    2.278 \t    0.120 \t     2.283 \t    0.111\n",
            "12 \t    2.265 \t    0.157 \t     2.273 \t    0.123\n",
            "13 \t    2.238 \t    0.212 \t     2.255 \t    0.186\n",
            "14 \t    2.176 \t    0.254 \t     2.216 \t    0.239\n",
            "Time elapsed (s): 26.2\n",
            "Epoch \tTrain_loss \tTrain_acc \tValid_loss \tValid_acc\n",
            "0 \t    2.304 \t    0.113 \t     2.304 \t    0.111\n",
            "1 \t    2.304 \t    0.113 \t     2.304 \t    0.111\n",
            "2 \t    2.303 \t    0.113 \t     2.304 \t    0.111\n",
            "3 \t    2.303 \t    0.113 \t     2.303 \t    0.111\n",
            "4 \t    2.302 \t    0.113 \t     2.303 \t    0.111\n",
            "5 \t    2.302 \t    0.113 \t     2.303 \t    0.111\n",
            "6 \t    2.302 \t    0.113 \t     2.302 \t    0.111\n",
            "7 \t    2.302 \t    0.113 \t     2.302 \t    0.111\n",
            "8 \t    2.301 \t    0.113 \t     2.302 \t    0.111\n",
            "9 \t    2.301 \t    0.113 \t     2.301 \t    0.111\n",
            "10 \t    2.301 \t    0.113 \t     2.301 \t    0.111\n",
            "11 \t    2.300 \t    0.113 \t     2.301 \t    0.111\n",
            "12 \t    2.300 \t    0.113 \t     2.301 \t    0.111\n",
            "13 \t    2.300 \t    0.113 \t     2.300 \t    0.111\n",
            "14 \t    2.300 \t    0.113 \t     2.300 \t    0.111\n",
            "Time elapsed (s): 25.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss for the different learning rates used\n",
        "Utility.plot_results([_[0] for _ in lr_results],\n",
        "                     labels=['lr=' + str(lr) for lr in lrs],\n",
        "                     ylabel='Cross-entropy loss',\n",
        "                     title='Effect of learning rate on training loss')"
      ],
      "metadata": {
        "id": "jqvZ1K3zp_SC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "556bd9a3-79a3-4ef4-ea6c-6a405caf7ed8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training accuracy for the different learning rates used\n",
        "Utility.plot_results([_[1] for _ in lr_results],\n",
        "                     labels=['lr=' + str(lr) for lr in lrs],\n",
        "                     ylabel='Accuracy', ymax=1.0,\n",
        "                     title='Effect of learning rate on training accuracy')"
      ],
      "metadata": {
        "id": "YhxS1S9PqsKq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "e9e89823-2afc-45e3-d882-3532821722b0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing the architecture\n",
        "\n",
        "*   Number of hidden layers\n",
        "*   Number of neurons per layer\n",
        "\n"
      ],
      "metadata": {
        "id": "pitXNowVqzBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP with 2 hidden layers\n",
        "p = Perceptron((width*width, 128, 64, 10),\n",
        "                  (Activation.relu, Activation.relu, Activation.softmax),\n",
        "                  Loss.cross_entropy, batch_size=100)\n",
        "\n",
        "# Run, store results in variable\n",
        "r2 = Utility.train_validate(p, mnist_set, lr=5e-4, epochs=25, verbose=True)\n",
        "\n",
        "# Plot results\n",
        "Utility.plot_results(r2[0], r2[1], r2[2], r2[3],\n",
        "                     labels=('Test loss', 'Test accuracy', 'Validation loss', \n",
        "                             'Validation accuracy'),\n",
        "                     fmts=('', '', 'o', 'o'), ymax=1.0,\n",
        "                     title='MLP with two hidden layers')"
      ],
      "metadata": {
        "id": "9YCf69NSnXPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP with 3 hidden layers\n",
        "p = Perceptron((width*width, 128, 64, 32, 10),\n",
        "               (Activation.relu, Activation.relu, Activation.relu, Activation.softmax),\n",
        "               Loss.cross_entropy, batch_size=100)\n",
        "r3 = Utility.train_validate(p, mnist_set, lr=5e-4, epochs=25, verbose=True)\n",
        "Utility.plot_results(r3[0], r3[1], r3[2], r3[3],\n",
        "                     labels=('Test loss', 'Test accuracy', 'Validation loss', \n",
        "                             'Validation accuracy'),\n",
        "                     fmts=('', '', 'o', 'o'), ymax=1.0,\n",
        "                     title='MLP with three hidden layers')"
      ],
      "metadata": {
        "id": "nUYB13p7pSC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP with 2 hidden layers, fewer neurons\n",
        "mlp = Perceptron((width*width, 16, 8, 10),\n",
        "                  (Activation.relu, Activation.relu, Activation.softmax),\n",
        "                  Loss.cross_entropy, batch_size=100)\n",
        "r2b = Utility.train_validate(mlp, mnist_set, lr=2e-3, epochs=25, verbose=True)\n",
        "Utility.plot_results(r2b[0], r2b[1], r2b[2], r2b[3],\n",
        "                     labels=('Test loss', 'Test accuracy', 'Validation loss', \n",
        "                             'Validation accuracy'),\n",
        "                     fmts=('', '', 'o', 'o'), ymax=1.0,\n",
        "                     title='MLP with two hidden layers, fewer neurons')"
      ],
      "metadata": {
        "id": "VPlcjxVgzj69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP with fewer neurons in hidden layers is more efficient to train but results in more loss and less accuracy. "
      ],
      "metadata": {
        "id": "mgMLR3t10yDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting activation functions"
      ],
      "metadata": {
        "id": "8LPlWqmM17Q4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing loss function"
      ],
      "metadata": {
        "id": "dnf7-wMhnl_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing how parameters are initialized"
      ],
      "metadata": {
        "id": "sZmoUz9un-2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Miscellaneous tasks\n",
        "\n",
        "Toy tests to manually check basic functionality"
      ],
      "metadata": {
        "id": "62TdC6i4TRvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hinge loss examples on 1/20/22 lecture:"
      ],
      "metadata": {
        "id": "FrMKJXoC_9wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-class classification example\n",
        "ex1 = np.array([-3.7, 5, 7]).T\n",
        "print('Target class is 3, loss={:0.1f} \\tif 2, {:0.1f} \\tif 1, {:0.1f}'.format(\n",
        "    Loss.hinge_loss(ex1, np.array([0, 0, 1]).T),\n",
        "    Loss.hinge_loss(ex1, np.array([0, 1, 0]).T),\n",
        "    Loss.hinge_loss(ex1, np.array([1, 0, 0]).T)))"
      ],
      "metadata": {
        "id": "nn7mgrr_ABYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7452b91-ea7d-4cb9-ec08-6b64e55c38dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target class is 3, loss=0.0 \tif 2, 3.0 \tif 1, 21.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-class classification example\n",
        "ex2 = np.array([2.5, 2.0, 2.7, 1.7]).T\n",
        "print('Target class is 2: {:0.1f}'.format(\n",
        "    Loss.hinge_loss(ex2, np.array([0, 1, 0, 0]).T)))"
      ],
      "metadata": {
        "id": "G3r5ejAXAEZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a4a381a-a8d4-4d84-fe24-90a788fec480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target class is 2: 3.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute accuracy on two examples, one correct and one not:"
      ],
      "metadata": {
        "id": "4jOLOwuQF6mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = np.array(((0.2, 0.5, 0.3), (0.7, 0.2, 0.1))).T\n",
        "y = np.array(((0, 1, 0), (0, 1, 0))).T\n",
        "Loss.accuracy(yhat, y)"
      ],
      "metadata": {
        "id": "EpN_cvn8F8_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b02d4761-73d4-4737-8c37-f93b674b7855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Informal problem given at the end of class, 1/25/22:"
      ],
      "metadata": {
        "id": "Fw_MM8svOBAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = Perceptron((4, 3, 2, 4), \n",
        "               (Activation.relu, Activation.sigmoid, Activation.softmax), \n",
        "               Loss.cross_entropy, init_with_normal=True)\n",
        "p[1].w = np.array([[1.,0,0,0], [0,1,0,0], [0,0,1,0]])\n",
        "p[2].w = np.array([[1.,0,0], [0,1,0]])\n",
        "p[3].w = np.array([[1.,0], [0,1], [0,0], [0,0]])\n",
        "p.zero_biases()\n",
        "\n",
        "# Check forward path\n",
        "p.forward(np.array([1, 0, 1, 0]).reshape(-1, 1), batch_size=1)\n",
        "for i in range(1, len(p.dims)):\n",
        "  print('Layer', i, '\\nz =\\n', p[i].z, '\\na =\\n', p[i].a, '\\n')"
      ],
      "metadata": {
        "id": "Z6ZL6w-XMa9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check backpropagation\n",
        "y = np.array([0., 1., 0., 0.]).reshape(-1, 1) # Dummy one-hot label\n",
        "print('p[1].w before backprop\\n', p[1].w)\n",
        "p.backward(y, lr=0.5, batch_size=1)\n",
        "print('p[1].w after backprop\\n', p[1].w)\n",
        "\n",
        "Activation.softmax(p[3].w @ p[2].a + p[3].b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDOfKgdXt6Gf",
        "outputId": "627d2d71-b63f-40dd-ea5f-0884326eadaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p[1].w before backprop\n",
            " [[1.04421637 0.         0.04421637 0.        ]\n",
            " [0.         1.         0.         0.        ]\n",
            " [0.03964693 0.         1.03964693 0.        ]]\n",
            "layer 1\n",
            "p[1].w after backprop\n",
            " [[1.14514486 0.         0.14514486 0.        ]\n",
            " [0.         1.         0.         0.        ]\n",
            " [0.09621592 0.         1.09621592 0.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.05991671],\n",
              "       [0.8446309 ],\n",
              "       [0.04772619],\n",
              "       [0.04772619]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    }
  ]
}