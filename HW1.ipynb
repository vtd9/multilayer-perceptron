{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "EeWhqjZ5-gcB",
        "t7v6weDTiymY",
        "uWe1jz6-4JoH",
        "I9ZZcb30LB9b",
        "gTQjTpXU-mC4",
        "aOpXBwW6FMLR",
        "pitXNowVqzBq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 1\n",
        "\n",
        "CS 637, Spring 2022\n",
        "\n",
        "Due 3/3/21\n",
        "\n",
        "1) Implement an API to create a fully-connected multilayer perceptron (MLP) for a *k*-class classification problem.\n",
        "\n",
        "2) Test this implementation on MNIST data."
      ],
      "metadata": {
        "id": "GnTdZZ4lrWeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notation\n",
        "\n",
        "*  `w`: weights\n",
        "*  `b`: biases\n",
        "*  `z`: aggregated values\n",
        "*  `a`: activated values\n",
        "*  `x`: input to a neural network\n",
        "*  `yhat`: final output from the neural network (i.e., *after* softmax if applied)"
      ],
      "metadata": {
        "id": "aB0N4lx1NH_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "metadata": {
        "id": "WDcWMVP-46qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. API"
      ],
      "metadata": {
        "id": "OHuM8L8m3E5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Layer class"
      ],
      "metadata": {
        "id": "EeWhqjZ5-gcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer(object):\n",
        "  '''\n",
        "  Represents one layer of a multilayer perceptron (MLP).\n",
        "  Initializes and updates parameter values (weights (w) and bias (b)) for \n",
        "  aggregation within the layer.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, in_dim=1, out_dim=1, activ_fun=None, init_with_normal=True, \n",
        "               mean=0, var=0.01, min=-0.1, max=0.1, a=None):\n",
        "    '''\n",
        "    Constructs a new Layer object.\n",
        "    @param in_dim: dimension of inputs to this layer\n",
        "    @param out_dim: dimension of outputs from this layer\n",
        "    @param activ_fun: collection of functions from the Activation class to \n",
        "    apply to each layer\n",
        "    @param init_with_normal: true to initialize parameters by sampling from a \n",
        "    normal distribution, false to sample from a uniform one\n",
        "    @param mean: mean to use if initializing with a normal dist\n",
        "    @param var: variance to use if initializing with a normal dist\n",
        "    @param min:\n",
        "    @param max:\n",
        "    @a: array to supply in special cases (i.e., input layer) to explicitly set \n",
        "    the activation values\n",
        "    '''\n",
        "    self.activ_fun = activ_fun\n",
        "    self.mean = mean\n",
        "    self.var = var\n",
        "    self.w = None\n",
        "    self.b = None\n",
        "    self.z = None\n",
        "    self.a = a\n",
        "\n",
        "    # Initialize layer's parameters if a non-input layer\n",
        "    if activ_fun is not None:\n",
        "      if init_with_normal:\n",
        "        self.normal_init(out_dim, in_dim, self.mean, self.var) # weights\n",
        "        self.normal_init(out_dim, 1, self.mean, self.var, True) # bias\n",
        "      else:\n",
        "        self.random_init(out_dim, in_dim, min, max) # weights\n",
        "        self.random_init(out_dim, 1, min, max, is_bias=True) # bias\n",
        "\n",
        "  def normal_init(self, out_dim, in_dim, mean=0.0, var=0.01, is_bias=False):\n",
        "    '''\n",
        "    Initialize parameter values by sampling from a normal distribution.\n",
        "    '''\n",
        "    # If variance not set, use Kaiming's initializaion with fan-in\n",
        "\n",
        "    # Initialize weights or bias\n",
        "    if is_bias:\n",
        "      self.b = np.random.normal(mean, var**0.5, (out_dim, in_dim))\n",
        "    else:\n",
        "      self.w = np.random.normal(mean, var**0.5, (out_dim, in_dim))\n",
        "\n",
        "  def random_init(self, out_dim, in_dim, min=-0.1, max=0.1, is_bias=False):\n",
        "    '''\n",
        "    Initialize parameter values with random values.\n",
        "    '''\n",
        "    if is_bias:\n",
        "      self.b = np.random.uniform(min, max, size=(out_dim, in_dim))\n",
        "    else:\n",
        "      self.w = np.random.uniform(min, max, size=(out_dim, in_dim))\n",
        "\n",
        "  def zero_bias(self):\n",
        "    '''\n",
        "    Set layer biases to zero.\n",
        "    '''\n",
        "    self.b = np.zeros(self.b.shape)\n",
        "  \n",
        "  def forward(self, input, batch_size):\n",
        "    '''\n",
        "    Pass inputs forward through the layer.\n",
        "    '''\n",
        "    # Aggregate\n",
        "    self.z = self.w @ input + self.b\n",
        "    if np.isinf(self.z).any(): \n",
        "      print(self.z, self.w, self.b)\n",
        "      raise ArithmeticError('Possibly exploding gradient')\n",
        "\n",
        "    # Activate\n",
        "    self.a = (self.activ_fun(self.z)).reshape(-1, batch_size)\n",
        "  \n",
        "  def adjust_weight(self, prev_a, grad_chain, lr, batch_size):\n",
        "    '''\n",
        "    Update weight values using the backpropagated gradient.\n",
        "    @param prev_a: activation of previous layer\n",
        "    @param grad_chain: backpropagated chain of derivatives so far\n",
        "    @param lr: learning rate\n",
        "    '''\n",
        "    # Normalize magnitude of adjustment with batch size\n",
        "    self.w -= lr/batch_size * (grad_chain @ prev_a.T)\n",
        "  \n",
        "  def adjust_bias(self, grad_chain, lr, batch_size):\n",
        "    '''\n",
        "    Update bias values using the backpropagated gradient.\n",
        "    @param grad_chain: backpropagated chain of derivatives so far\n",
        "    @param lr: learning rate\n",
        "    '''\n",
        "    # Derivative of an aggregate wrt bias (dz/db) = 1, so don't need the \n",
        "    # activation values of the previous layer\n",
        "    self.b -= lr/batch_size * np.sum(grad_chain, axis=1).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "TXeX29VN5ZYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Perceptron class"
      ],
      "metadata": {
        "id": "z-GYqijcKkcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron(object):\n",
        "  '''\n",
        "  Represents a multi-layer perceptron (MLP) for processing input data. Made of\n",
        "  an aggregate of Layer objects.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, dims, activ_fns, loss_fn,\n",
        "               init_with_normal=True, mean=0.0, var=0.01, min=-0.1, max=0.1):\n",
        "    '''\n",
        "    Constructs a new Perceptron object.\n",
        "\n",
        "    @param dims: array of ints, each representing the dimensionality of layer\n",
        "    of the same index. First int is number of input features; last int\n",
        "    is number of output neurons.\n",
        "    @param activ_fns: list or tuple of methods from the Activation class to \n",
        "    apply on each hidden layer and the output layer\n",
        "    @param loss_fn: loss function to measure model performance\n",
        "    @param init_with_normal: true to initialize parameters by sampling from a \n",
        "    normal distribution, false to sample from a uniform one\n",
        "    @param mean: mean to use if initializing with a normal distribution\n",
        "    @param var: variance to use if initializing with a normal distribution\n",
        "    @param min: minimum bound if initializing with a uniform distribution\n",
        "    @param max: maximum bound if initializing with a uniform distribution\n",
        "    '''\n",
        "    # Ensure Perceptron has at least input and output layers\n",
        "    assert len(dims) > 1\n",
        "\n",
        "    # Ensure amount of activation functions given matches number of layers - 1\n",
        "    assert len(dims) == len(activ_fns) + 1\n",
        "    \n",
        "    self.dims = dims\n",
        "    self.n_layers = len(dims)\n",
        "    self.activ_fns = activ_fns\n",
        "    self.loss_fn = loss_fn\n",
        "    self.reset_layers(init_with_normal, mean, var, min, max)\n",
        "\n",
        "  def reset_layers(self, init_with_normal=True, mean=0.0, var=0.01, \n",
        "                   min=-0.1, max=0.1):  \n",
        "    '''\n",
        "    Initialize parameters in the MLP.\n",
        "    '''\n",
        "    self.layers = [None] * len(self.dims)\n",
        "\n",
        "    # For each layer besides the input layer, attach the appropriate dimensions, \n",
        "    # activation function, and initialization parameters\n",
        "    for i in range(self.n_layers - 1):\n",
        "      self.layers[i + 1] = Layer(\n",
        "          in_dim=self.dims[i], out_dim=self.dims[i+1],\n",
        "          activ_fun=self.activ_fns[i], \n",
        "          init_with_normal=init_with_normal,\n",
        "           mean=mean, var=var, min=min, max=max)\n",
        "  \n",
        "  def zero_biases(self):\n",
        "    '''\n",
        "    Set all biases in the MLP to zero.\n",
        "    '''\n",
        "    for i in range(1, self.n_layers):\n",
        "      self.layers[i].zero_bias()\n",
        "\n",
        "  def __getitem__(self, layer_number):\n",
        "    '''\n",
        "    Retrieves reference to a layer in the MLP.\n",
        "\n",
        "    @param layer_number: int index of layer to access.\n",
        "    @return: Layer object at layer_number\n",
        "    '''\n",
        "    return self.layers[layer_number]\n",
        "  \n",
        "  def forward(self, X, batch_size):\n",
        "    '''\n",
        "    Step forward through the MLP, aggregating the previous layer's values and\n",
        "    then applying the chosen activation function.\n",
        "\n",
        "    @param X: input data to feed through the network\n",
        "    @param batch size\n",
        "    '''\n",
        "    # Set the first layer (the input layer)'s \"activation\" to the input data\n",
        "    self.layers[0] = Layer(a=X)\n",
        "\n",
        "    # Pass through each layer using inputs from previous layer\n",
        "    for i, layer in enumerate(self.layers[1:], start=1):\n",
        "      layer.forward(self.layers[i-1].a, batch_size)\n",
        "\n",
        "      # TEMP\n",
        "      if np.isnan(self.layers[i].a).any():\n",
        "        print(np.max(self.layers[i].w), np.max(self.layers[i].b), self.layers[i-1].a)\n",
        "        raise Exception('Activation NaN during forward pass!!!')\n",
        " \n",
        "  \n",
        "  def backward(self, labels, lr, batch_size):\n",
        "    '''\n",
        "    Backpropagates from loss and adjust parameters after a forward pass.\n",
        "    \n",
        "    @param labels: true labels from the input data\n",
        "    @param lr: learning rate\n",
        "    @param batch_size: batch size\n",
        "    '''\n",
        "    # Calculate derivative of loss wrt logits\n",
        "    grad_chain = self.loss_fn(self.layers[-1].a, labels, derive=True)\n",
        "\n",
        "    for i in range(self.n_layers - 1, 0, -1):\n",
        "      # Save current weight for dloss/da of the previous layer later\n",
        "      prev_a = self.layers[i-1].a\n",
        "      old_weight = self.layers[i].w\n",
        "\n",
        "      # Adjust parameters\n",
        "      self.layers[i].adjust_weight(prev_a, grad_chain, lr, batch_size)\n",
        "      self.layers[i].adjust_bias(grad_chain, lr, batch_size)\n",
        "\n",
        "      # Compute dloss/da of previous layer, multiply to current product\n",
        "      grad_chain = (old_weight.T @ grad_chain) \n",
        "\n",
        "      # Compute dloss/dz of previous layer using elementwise multiplication\n",
        "      if self.layers[i-1].activ_fun is not None:\n",
        "        da_dz = self.layers[i-1].activ_fun(prev_a, derive=True)\n",
        "      else: # If hit the input layer\n",
        "        break\n",
        "        \n",
        "      grad_chain = grad_chain * da_dz\n",
        "      if np.isnan(grad_chain).any() or np.isinf(grad_chain).any():\n",
        "        raise Exception('Exploded grad_chain!!!')\n",
        "  \n",
        "  def train(self, iterator, lr, batch_size, train_mode=True):\n",
        "    '''\n",
        "    Train for a single epoch.\n",
        "    @param iterator: object containing batch generator to loop through\n",
        "    @param lr: learning rate\n",
        "    @param train_mode: true to backpropagate and upate parameters, false to not\n",
        "    '''\n",
        "    loss = 0.0\n",
        "    acc = 0.0\n",
        "    for i, (X, y) in enumerate(iterator):\n",
        "      self.forward(X, batch_size)\n",
        "\n",
        "      if train_mode: # Adjust parameters only during training\n",
        "        self.backward(y, lr, batch_size)\n",
        "\n",
        "      # Update accumulated measures\n",
        "      print(self.loss_fn(self[-1].a, y)) # TEMP\n",
        "      loss += self.loss_fn(self[-1].a, y)\n",
        "      acc += Loss.accuracy(self[-1].a, y)\n",
        "\n",
        "    # Average over the number of batches done\n",
        "    return loss/(i+1), acc/(i+1)"
      ],
      "metadata": {
        "id": "OCaLYwcnou37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Activation and loss functions"
      ],
      "metadata": {
        "id": "t7v6weDTiymY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation(object):\n",
        "  '''\n",
        "  Common activation functions used in a MLP.\n",
        "  '''\n",
        "  @staticmethod\n",
        "  def relu(x, derive=False):\n",
        "    '''\n",
        "    Applies ReLU or its derivative on an input.\n",
        "    @param x: input data\n",
        "    '''\n",
        "    if not derive:\n",
        "      return np.maximum(x, 0., x)\n",
        "    else:\n",
        "      return (x > 0).astype(int)\n",
        "  \n",
        "  @staticmethod\n",
        "  def sigmoid(x, derive=False):\n",
        "    '''\n",
        "    Applies sigmoid or its deriative on an input.\n",
        "    @param x: input data\n",
        "    '''\n",
        "    if not derive:\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "    else: \n",
        "      return Activation.sigmoid(x) * (1-Activation.sigmoid(x))\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def softmax(x):\n",
        "    '''\n",
        "    Applies softmax on an input.\n",
        "    @param x: input data\n",
        "    @return: normalized values of the input data so sum of a given example = 1\n",
        "    '''\n",
        "    # Update from using a naive implementation - prevent numerical overflows\n",
        "    # by using shifting trick\n",
        "    # return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "    shift = np.max(x)\n",
        "    exp_x = np.exp(x - shift)\n",
        "    return exp_x / np.sum(exp_x, axis=0)"
      ],
      "metadata": {
        "id": "5Ih0IHs_ixjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss(object):\n",
        "  @staticmethod\n",
        "  def cross_entropy(yhat, y, epsilon=1e-5, derive=False):\n",
        "    '''\n",
        "    Computes the cross-entropy loss or its gradient with respect to logits for \n",
        "    a batch of predictions.\n",
        "    @param yhat: final outputs (after activation) from a MLP\n",
        "    @param y: actual labels to compare output with\n",
        "    @param epsilon: small positive value to prevent taking log of 0\n",
        "    '''\n",
        "    if not derive:\n",
        "      # Squash for EACH example by summing before averaging - \n",
        "      # erroneously many zeros otherwise!\n",
        "      return np.average(np.sum(-y * np.log(yhat + epsilon), axis=0))\n",
        "    else:\n",
        "      return yhat - y\n",
        "\n",
        "  @staticmethod\n",
        "  def hinge_loss(yhat, y, margin=1, get_dist=False, derive=False):\n",
        "    '''\n",
        "    Computes the hinge loss or its derivative for a batch of predictions.\n",
        "    @param yhat: final outputs (after activation) from a MLP\n",
        "    @param y: actual labels to compare output with\n",
        "    @param margin: desired margin for target class value to be overtake\n",
        "    @param get_dist: true to return raw distances for each sample, false to\n",
        "    return actual value of hinge loss\n",
        "    '''\n",
        "    # Output of the target class for each example\n",
        "    # Repeat so same value going down row to get same shape as yhat\n",
        "    y_target = np.sum(np.where(y == 1, yhat, 0), axis=0)\n",
        "\n",
        "    # Get distances from outputs at indexes != target class\n",
        "    # Add margin to elements at indexes != target class\n",
        "    dist = np.where(y == 1, 0, yhat - y_target + margin)\n",
        "\n",
        "    # If need raw distances before thresholding/averaging:\n",
        "    if get_dist:\n",
        "      return dist\n",
        "\n",
        "    if not derive:\n",
        "      # Get the positive differences over each example (zero-threshold)\n",
        "      pos_per_example = np.where(dist > 0, dist, 0)\n",
        "\n",
        "      # Sum over each example, average over all batches\n",
        "      return np.sum(np.sum(pos_per_example, axis=0))\n",
        "    else:\n",
        "      return\n",
        "  \n",
        "  @staticmethod\n",
        "  def accuracy(yhat, y, y_one_hot=True):\n",
        "    '''\n",
        "    Computes the accuracy for a batch of outputs.\n",
        "    @param yhat: final outputs (after activation) from a MLP\n",
        "    @param y: actual labels to compare output with\n",
        "    @param y_one_hot: true if y given in one-hot form, false if given as a list \n",
        "    of correct indexes\n",
        "    '''\n",
        "    # Prediction class is the one with max predicted probability in each example\n",
        "    yhat_choices = np.argmax(yhat, axis=0)\n",
        "\n",
        "    # Reverse the actual labels' representation as one-hot for easier comparison\n",
        "    if y_one_hot:\n",
        "      y = Utility.reverse_one_hot(y)\n",
        "\n",
        "    # Compute the average accuracy of the batch\n",
        "    return np.average(yhat_choices == y)"
      ],
      "metadata": {
        "id": "cKXFSrnTBkys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMP: hinge loss bumbling\n",
        "yhat = np.random.normal(size=(4,3)).T\n",
        "y = np.zeros((4, 3)).T\n",
        "y[0, 0] = 1\n",
        "y[2, 1] = 1\n",
        "y[1, 2] = 1\n",
        "y[0, 3] = 1\n",
        "yhat, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQyT3KsAJxc3",
        "outputId": "a75e6802-8161-4ed0-ea26-e87cfd7fa921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 1.10037243, -0.70922258,  0.39471252, -0.01248606],\n",
              "        [-2.00038539, -1.70548285, -0.27780035, -0.37525978],\n",
              "        [-0.45434826,  1.44905675, -0.82113996, -0.01582385]]),\n",
              " array([[1., 0., 0., 1.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 1., 0., 0.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMP\n",
        "a = Loss.hinge_loss(yhat, y, get_dist=True)\n",
        "maybe_deriv = np.where(y == 1, np.sum(a > 0, axis=0), a > 0)\n",
        "a, maybe_deriv"
      ],
      "metadata": {
        "id": "iWaqnbauNR8q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df67fb9d-148b-4b57-f08e-68f1d9c107ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0.        , -1.15827934,  1.67251288,  0.        ],\n",
              "        [-2.10075782, -2.15453961,  0.        ,  0.63722628],\n",
              "        [-0.55472069,  0.        ,  0.45666039,  0.99666221]]),\n",
              " array([[0, 0, 1, 2],\n",
              "        [0, 0, 2, 1],\n",
              "        [0, 0, 1, 1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Utilities class"
      ],
      "metadata": {
        "id": "uWe1jz6-4JoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Utility(object):\n",
        "  '''\n",
        "  Miscellaneous methods for wrapping and plotting.\n",
        "  '''\n",
        "  @staticmethod\n",
        "  def train_validate(mlp, dataset, lr, epochs, batch_size,\n",
        "                     shuffle_every_epoch=True, verbose=True):\n",
        "    '''\n",
        "    Train and validate in parallel over one or more epochs.\n",
        "    @param mlp: Perceptron object with parameters initialized\n",
        "    '''\n",
        "    assert (epochs > 0)\n",
        "    train_loss = np.empty(epochs)\n",
        "    train_acc = np.empty(epochs)\n",
        "    valid_loss = np.empty(epochs)\n",
        "    valid_acc = np.empty(epochs)\n",
        "    start_time = time.time()\n",
        "    if verbose:\n",
        "      print('Epoch \\tTrain_loss \\tTrain_acc \\tValid_loss \\tValid_acc')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      # Make iterators\n",
        "      train_iter = dataset.make_batches(batch_size, group='train', \n",
        "                                        shuffle_again=shuffle_every_epoch)\n",
        "      valid_iter = dataset.make_batches(batch_size, group='valid',\n",
        "                                        shuffle_again=shuffle_every_epoch)\n",
        "\n",
        "      # Train and validate - validate \"previous\" model before updating\n",
        "      valid_loss[epoch], valid_acc[epoch] = mlp.train(valid_iter, lr=lr,\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      train_mode=False)\n",
        "      train_loss[epoch], train_acc[epoch] = mlp.train(train_iter, lr=lr,\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      train_mode=True)\n",
        "      \n",
        "      # If set to print out info as executing\n",
        "      if verbose:\n",
        "        print('{} \\t{:9.3f} \\t{:9.3f} \\t {:9.3f} \\t{:9.3f}'.format(\n",
        "            epoch, train_loss[epoch], train_acc[epoch], \n",
        "            valid_loss[epoch], valid_acc[epoch]))\n",
        "\n",
        "    # Print amount of time taken\n",
        "    print('Time elapsed (s): {:2.1f}'.format(time.time() - start_time))\n",
        "\n",
        "\n",
        "    return train_loss, train_acc, valid_loss, valid_acc\n",
        "  \n",
        "  @staticmethod\n",
        "  def make_one_hot(y):\n",
        "    '''\n",
        "    Make a list/collection of labels into a one-hot representation.\n",
        "    @param y: list/collection of labels to convert\n",
        "    @return transformed array in one-hot representation\n",
        "    '''\n",
        "    transformed = np.zeros((y.size, y.max() + 1))\n",
        "    transformed[np.arange(y.size), y] = 1\n",
        "    return transformed.T\n",
        "\n",
        "  @staticmethod\n",
        "  def reverse_one_hot(y):\n",
        "    '''\n",
        "    Convert a one-hot array back to a simple list of indices.\n",
        "    @param y: one-hot array to convert\n",
        "    @return: transformed array in list representation\n",
        "    '''\n",
        "    return np.argmax(y, axis=0)\n",
        "\n",
        "  @staticmethod\n",
        "  def plot_images(dataset, y, num_images=3):\n",
        "    '''\n",
        "    Plot using the flattened representation of a square image in input X.\n",
        "    \n",
        "    '''\n",
        "    # Get random sample of dataset\n",
        "    indexes = np.random.randint(0, dataset.X.shape[-1], num_images)\n",
        "\n",
        "    # Display data as images\n",
        "    width = int(dataset.X.shape[0]**0.5)\n",
        "    for i in indexes:  \n",
        "      plt.subplot(330 + 1 + i)\n",
        "      plt.axis('off')\n",
        "      plt.imshow(dataset.X[:, i+i].reshape(width, width), \n",
        "                 cmap=plt.get_cmap('gray'))\n",
        "      \n",
        "      # Title each image with its true label\n",
        "      plt.title([Utility.reverse_one_hot(y[:, i+i])])\n",
        "\n",
        "      # TItle ach image with its predicted label, if any\n",
        "  \n",
        "  @staticmethod\n",
        "  def plot_results(*results, labels=None, fmts=None, xlabel='Epoch', ylabel='',\n",
        "                   xticks=None, ymax=None, title=''):\n",
        "    # Account for arguments not given\n",
        "    if labels is None: \n",
        "      labels = (str(_) for _ in range(len(results)))\n",
        "    if fmts is None:\n",
        "      fmts = ('' for _ in range(len(results)))\n",
        "\n",
        "    # Plot each set of results with its corresponding label and format\n",
        "    for result, label, fmt in zip(results, labels, fmts):\n",
        "      plt.plot(np.arange(result.shape[0]), result, fmt, label=label)\n",
        "\n",
        "    # Parameters affecting entire plot\n",
        "    plt.legend()\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    if xticks is not None:\n",
        "      plt.xticks(np.arange(results[0].shape[0]))    \n",
        "    if ymax is not None:\n",
        "      plt.ylim(0, ymax)"
      ],
      "metadata": {
        "id": "QMGZb_cJCAL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Dataset class"
      ],
      "metadata": {
        "id": "I9ZZcb30LB9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(object):\n",
        "  '''\n",
        "  Represents a set of data with methods to reshape, split, and train.\n",
        "  '''\n",
        "  def __init__(self, X, y, make_y_one_hot=True):\n",
        "    '''\n",
        "    Constructs a Dataset object.\n",
        "    @param X: input data\n",
        "    @param y: labels corresponding to input data\n",
        "    @param make_y_one_hot: true to convert the labels into a one-hot \n",
        "    representation, if not done already; false to skip this step\n",
        "    '''\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    if make_y_one_hot:\n",
        "      self.y = Utility.make_one_hot(self.y)\n",
        "\n",
        "  def shape(self, features, categories, flatten_X=True):\n",
        "    '''\n",
        "    Reshapes the input and output data.\n",
        "    @param features: number of features in the input data. Set to width \n",
        "    if a 2D image that will be flattened.\n",
        "    @param categories:\n",
        "    @flatten_X: true to flatten a 2D image into a 1D array\n",
        "    '''\n",
        "    if flatten_X: \n",
        "      # Make 2D images into an array of n x features^2, where n is the total\n",
        "      # number of examples\n",
        "      self.X = self.X.reshape(-1, features*features).T\n",
        "    else:\n",
        "      self.X = self.X.reshape(features, -1)\n",
        "    self.y = self.y.reshape(categories, -1)\n",
        "\n",
        "  def shuffle(self):\n",
        "    '''\n",
        "    Shuffles both X and y together in the dataset.\n",
        "    '''\n",
        "    assert self.X.shape[-1] == self.y.shape[-1]\n",
        "    shuffled_indexes = np.random.permutation(self.X.shape[-1])\n",
        "    self.X = self.X[:, shuffled_indexes]\n",
        "    self.y = self.y[:, shuffled_indexes]\n",
        "\n",
        "  def divide(self, p_train=70, p_valid=15, p_test=15):\n",
        "    '''\n",
        "    Divide the loaded data into sets for training, validation, and testing.\n",
        "    @param p_train: proportion of dataset to allot for training\n",
        "    @param p_valid: proportion of dataset to allot for validation\n",
        "    @param p_test: proportion of dataset to allot for testing\n",
        "    '''\n",
        "    if (p_train + p_valid + p_test != 100):\n",
        "      raise ValueError('Error: percentages don''t sum up to 100!')\n",
        "    self.n_train = int(p_train * 0.01 * self.X.shape[-1])\n",
        "    self.n_valid = int(p_valid * 0.01 * self.X.shape[-1])\n",
        "    self.n_test = int(p_test * 0.01 * self.X.shape[-1])\n",
        "\n",
        "    # If missing a few examples from cutoffs, add to training set\n",
        "    test_diff = self.X.shape[-1] - (self.n_train + self.n_valid + self.n_test)\n",
        "    if test_diff > 0:\n",
        "      self.n_train += test_diff\n",
        "\n",
        "    # Define groups\n",
        "    self.X_train = self.X[:, 0:self.n_train]\n",
        "    self.y_train = self.y[:, 0:self.n_train]\n",
        "    self.X_valid = self.X[:, self.n_train:(self.n_train + self.n_valid)]\n",
        "    self.y_valid = self.y[:, self.n_train:(self.n_train + self.n_valid)]\n",
        "    self.X_test = self.X[:, (self.n_train + self.n_valid):]\n",
        "    self.y_test = self.y[:, (self.n_train + self.n_valid):]\n",
        "    \n",
        "  def make_batches(self, batch_size, group='train', scale_X=True,\n",
        "                   shuffle_again=False):\n",
        "    '''\n",
        "    Make batches given a specified size and a selected group.\n",
        "    @param batch_size: int of batch size\n",
        "    @param group: string equal to train, valid, or test to select group\n",
        "    @param scale_X: true to rescale \n",
        "    '''\n",
        "    # Get the training, testing, or validation group:\n",
        "    if group == 'train':\n",
        "      X_select, y_select = self.X_train, self.y_train\n",
        "    elif group == 'valid':\n",
        "      X_select, y_select = self.X_valid, self.y_valid\n",
        "    elif group == 'test':\n",
        "      X_select, y_select = self.X_test, self.y_test\n",
        "    else:\n",
        "      raise ValueError('Incorrect argument for group! Choose between train, '\n",
        "                       'validate, or test.')\n",
        "\n",
        "    # Shuffle within the group, if requested\n",
        "    if shuffle_again:\n",
        "      shuffled_indexes = np.random.permutation(X_select.shape[-1])\n",
        "      X_select = X_select[:, shuffled_indexes]\n",
        "      y_select = y_select[:, shuffled_indexes]\n",
        "\n",
        "    # Make generators for the features and labels of the selected set\n",
        "    for i in range(0, X_select.shape[-1], batch_size):\n",
        "      if scale_X: # Rescale X to have values between 0 and 1\n",
        "        X_scaled = np.linalg.norm(X_select[:, i:i+batch_size])\n",
        "        yield X_select[:, i:i+batch_size]/X_scaled, y_select[:, i:i+batch_size]\n",
        "      else:\n",
        "        yield X_select[:, i:i+batch_size], y_select[:, i:i+batch_size]"
      ],
      "metadata": {
        "id": "hp-u4Odi4MHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Testing the API"
      ],
      "metadata": {
        "id": "ejKqz_Tv2A3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Import the MNIST dataset\n",
        "*   Permitted to use other packages for just downloading the data, so use `keras` to download.\n",
        "*   Merge existing training and test sets and resplit, since splitting must be done by this homework's API.\n",
        "\n"
      ],
      "metadata": {
        "id": "gTQjTpXU-mC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
      ],
      "metadata": {
        "id": "UFHR62sWB4eT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c66fe7d-d46f-4c9f-fc07-f9b25c536bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_all_X = np.concatenate((train_X, test_X), axis=0)\n",
        "mnist_all_y = np.concatenate((train_y, test_y), axis=0)\n",
        "print('MNIST features shape:', mnist_all_X.shape, \n",
        "      '\\nMNIST label shape:', mnist_all_y.shape)"
      ],
      "metadata": {
        "id": "jqrK7cd-E0BJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f6b6c28-38f6-4376-e519-e49c6b40ce05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST features shape: (70000, 28, 28) \n",
            "MNIST label shape: (70000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Preprocessing"
      ],
      "metadata": {
        "id": "aOpXBwW6FMLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a Dataset object\n",
        "mnist_data = Dataset(mnist_all_X, mnist_all_y, True)\n",
        "\n",
        "# Reshape the data\n",
        "width = 28\n",
        "categories = 10\n",
        "mnist_data.shape(width, categories, True)\n",
        "print(\"Feature and label shapes:\", mnist_data.X.shape, mnist_data.y.shape)\n",
        "\n",
        "# Shuffle the data\n",
        "# plot first __ before shuffling\n",
        "mnist_data.shuffle()\n",
        "# plot first __ after to prove it's shuffled?\n",
        "\n",
        "# Divide the data\n",
        "mnist_data.divide()\n",
        "print('Splits on training, validation, & testing:', \n",
        "      mnist_data.n_train, mnist_data.n_valid, mnist_data.n_test)"
      ],
      "metadata": {
        "id": "cueDPZa2qKnz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "266e7ce2-58c5-4ee5-a06d-60e396cea33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature and label shapes: (784, 70000) (10, 70000)\n",
            "Splits on training, validation, & testing: 49000 10500 10500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and validation"
      ],
      "metadata": {
        "id": "pH2B2SnNKrVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varying the learning rate"
      ],
      "metadata": {
        "id": "D6yuzi4ey9Cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_grad = np.arange(5)\n",
        "clipped = test_grad/np.linalg.norm(test_grad)\n",
        "clipped, np.linalg.norm(clipped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR72MyLMN5iW",
        "outputId": "45f64a0b-3b85-4d3a-990b-c720ec00acf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.        , 0.18257419, 0.36514837, 0.54772256, 0.73029674]),\n",
              " 0.9999999999999999)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a generic Percentron object, reset every epoch\n",
        "p = Perceptron((width*width, 8, 10),\n",
        "                 (Activation.relu, Activation.softmax),\n",
        "                 Loss.cross_entropy)\n",
        "learning_rates = (0.1,)\n",
        "lr_results = []\n",
        "p.train(mnist_data.make_batches(batch_size=6), 0.1, 100)"
      ],
      "metadata": {
        "id": "kEKW_IuKy_an",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7d64761-437a-4408-9129-c0fc8bcef188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31.838093005090435\n",
            "30.02463005259264\n",
            "29.16313002990333\n",
            "29.775236570482406\n",
            "28.185288637096967\n",
            "27.16190358803683\n",
            "29.002209478138212\n",
            "27.83267842695235\n",
            "26.730222937579107\n",
            "26.761126415134775\n",
            "28.653074874685796\n",
            "24.803088000115267\n",
            "24.722400350658265\n",
            "27.181333832647454\n",
            "23.820205340229627\n",
            "26.16699825841436\n",
            "24.824606112023744\n",
            "25.162803009539477\n",
            "25.753990240400146\n",
            "24.38535414208958\n",
            "26.1073703315452\n",
            "23.737548735810783\n",
            "23.231364684920443\n",
            "23.781436752490016\n",
            "23.896371900607743\n",
            "23.39384361996373\n",
            "24.53078782684188\n",
            "23.304553120823154\n",
            "23.808165094093564\n",
            "25.032569982655538\n",
            "24.175982597703847\n",
            "23.239199582962023\n",
            "23.588747819376575\n",
            "23.52168170896497\n",
            "23.928043073398122\n",
            "23.322155495035123\n",
            "24.08728451545279\n",
            "23.914471172527875\n",
            "23.608083044705705\n",
            "23.08009192668867\n",
            "22.928932785214037\n",
            "23.14825264807493\n",
            "23.97275998922659\n",
            "23.297456376403165\n",
            "23.454190814885706\n",
            "23.18215704452139\n",
            "22.741730433702106\n",
            "23.29169955938186\n",
            "23.20854165638695\n",
            "23.156947865956845\n",
            "23.590949929696915\n",
            "22.99183742481079\n",
            "23.168401017103587\n",
            "23.223120702858306\n",
            "22.74148138383883\n",
            "23.159896235421897\n",
            "23.3201454092764\n",
            "23.535375774445207\n",
            "23.2691318068561\n",
            "23.206738789793132\n",
            "23.026604099913083\n",
            "23.27891234202382\n",
            "22.839113557562374\n",
            "22.961859921171836\n",
            "23.05479317348958\n",
            "23.10226771134682\n",
            "23.034253291096487\n",
            "22.716749802053258\n",
            "23.23633435020864\n",
            "22.965571244030002\n",
            "22.95252135401019\n",
            "23.015697983230368\n",
            "22.986919347216357\n",
            "23.355716444137887\n",
            "23.040419232546633\n",
            "23.157009298282976\n",
            "23.089393014037523\n",
            "22.894245622269487\n",
            "23.026452527049194\n",
            "22.97153166596538\n",
            "23.01006010857049\n",
            "23.051234454399097\n",
            "23.111920353368987\n",
            "23.017570838182742\n",
            "23.21743313446881\n",
            "22.965715276070647\n",
            "23.07176962060048\n",
            "23.074206540089072\n",
            "22.744860142634373\n",
            "22.950943447287887\n",
            "23.13683017728031\n",
            "22.948872569193874\n",
            "22.78742798768807\n",
            "23.20900444116201\n",
            "23.007113821905254\n",
            "22.90558501938052\n",
            "23.11452794725646\n",
            "23.154561286462787\n",
            "22.98585004207567\n",
            "22.982050726746824\n",
            "23.133439344638436\n",
            "23.032641899016973\n",
            "22.91904443537505\n",
            "22.95032178716785\n",
            "23.014968951090246\n",
            "23.00217299157179\n",
            "22.851961830686648\n",
            "23.005855590281975\n",
            "22.873759188463588\n",
            "23.076105641486713\n",
            "22.878731512466402\n",
            "22.96957423540158\n",
            "22.94162250162531\n",
            "23.154464352284418\n",
            "22.948051739375682\n",
            "22.970362727848062\n",
            "22.840553487899108\n",
            "23.0896340760578\n",
            "23.13355522779984\n",
            "22.960498878628478\n",
            "23.143402043557415\n",
            "22.963864885662126\n",
            "23.113512093996512\n",
            "22.84874149229899\n",
            "23.10052989778764\n",
            "23.109846383308355\n",
            "22.96180778967898\n",
            "23.089014987325484\n",
            "23.08357391737129\n",
            "23.10503259707106\n",
            "23.065494360779176\n",
            "23.05685895808023\n",
            "22.960429443789423\n",
            "22.962606412879772\n",
            "23.049733623693278\n",
            "23.017486573606085\n",
            "22.974669392596947\n",
            "23.193291550105478\n",
            "22.97888700032417\n",
            "22.98791673162011\n",
            "23.075765231582686\n",
            "23.012652456810137\n",
            "23.080450134534964\n",
            "22.94707923548399\n",
            "22.92793089485445\n",
            "22.89223332982363\n",
            "23.136473079548985\n",
            "22.93482984930256\n",
            "22.99969138924197\n",
            "22.90173297960687\n",
            "22.992183669790506\n",
            "22.90774766589408\n",
            "22.960371776567904\n",
            "23.028503342685887\n",
            "22.958014780290988\n",
            "22.92206582632438\n",
            "23.081591829894908\n",
            "23.16663234264174\n",
            "23.019327336130154\n",
            "23.048192580467845\n",
            "23.057009650034466\n",
            "22.911672014981995\n",
            "23.08888019554831\n",
            "23.05660226856842\n",
            "22.937703585851047\n",
            "23.080650144423394\n",
            "23.017345424600517\n",
            "22.81236202251461\n",
            "22.70078513615822\n",
            "23.033226944461994\n",
            "23.148423057220366\n",
            "23.157120348464264\n",
            "22.927204339503604\n",
            "23.127387676475944\n",
            "23.00139241153505\n",
            "22.896579352104748\n",
            "23.06560178683749\n",
            "22.99415282505687\n",
            "22.97848059087509\n",
            "22.982410430008713\n",
            "23.05198776942145\n",
            "22.919112216494277\n",
            "22.941673842577934\n",
            "23.079092203951102\n",
            "23.09873928861414\n",
            "23.139145513065618\n",
            "23.11520545046747\n",
            "22.906064052379456\n",
            "22.898958634893752\n",
            "22.920535300945975\n",
            "23.13642161068086\n",
            "23.030367841563812\n",
            "22.989753764834514\n",
            "23.02243858456326\n",
            "23.110103304907305\n",
            "22.853459705284706\n",
            "23.07297446080687\n",
            "23.005414360243627\n",
            "22.93170949326719\n",
            "23.061020352053763\n",
            "22.89643310415078\n",
            "22.835294064771574\n",
            "22.91085466648956\n",
            "23.08178180452997\n",
            "23.071162406251943\n",
            "23.09643572336139\n",
            "22.921684654013305\n",
            "23.125768570250226\n",
            "23.04279793714366\n",
            "23.112201294770028\n",
            "23.05737794158753\n",
            "22.973748279799253\n",
            "22.97575679796304\n",
            "23.049512255765457\n",
            "22.97792329590765\n",
            "23.046087529613356\n",
            "23.05502151463904\n",
            "22.885264776084306\n",
            "23.1050698444701\n",
            "22.980854113691088\n",
            "22.954792558221442\n",
            "23.041036208254507\n",
            "22.984510803309455\n",
            "23.07030950675648\n",
            "23.019096207908422\n",
            "22.975888655074783\n",
            "23.06023633801899\n",
            "23.10969833749438\n",
            "22.978861512508587\n",
            "23.017766986187475\n",
            "23.006641030526765\n",
            "23.08388742705742\n",
            "23.03141824192233\n",
            "22.909256489558935\n",
            "23.09004325952913\n",
            "23.094708039186695\n",
            "22.9548571513642\n",
            "23.048805190887244\n",
            "22.940928921551755\n",
            "22.92000610152997\n",
            "22.94329878080811\n",
            "23.22927687043202\n",
            "22.94299400484964\n",
            "22.93166190019417\n",
            "22.865830063310625\n",
            "22.922070502973778\n",
            "23.046267170880718\n",
            "23.028214276130175\n",
            "22.933213711673\n",
            "23.127469304503414\n",
            "23.0844123597152\n",
            "23.01731084246563\n",
            "23.024491713637666\n",
            "23.095331275863945\n",
            "23.04211961157757\n",
            "23.06374914499442\n",
            "23.02875173719116\n",
            "22.970121510069994\n",
            "23.039960233310698\n",
            "23.070935232140776\n",
            "23.07404098184116\n",
            "22.954857731738333\n",
            "23.045988374299572\n",
            "23.059477286243506\n",
            "23.052202660241345\n",
            "23.010792572098868\n",
            "22.94006265369439\n",
            "22.99685501059032\n",
            "23.061505907063427\n",
            "22.93637906289943\n",
            "23.16865928586246\n",
            "23.054055949710584\n",
            "22.934264422688557\n",
            "23.087025952469443\n",
            "22.966983251007917\n",
            "23.068549741662586\n",
            "22.974215881314695\n",
            "22.99127640877517\n",
            "22.990950125679678\n",
            "23.00249542424558\n",
            "23.102905706805462\n",
            "23.014985362690282\n",
            "23.015037185763298\n",
            "23.001176912135737\n",
            "23.04883428942117\n",
            "22.98056736226275\n",
            "22.923529445712877\n",
            "23.030083685594246\n",
            "22.93717400926959\n",
            "23.04440612033469\n",
            "22.865513345690367\n",
            "22.99489909482902\n",
            "22.973964442849706\n",
            "22.99036799581105\n",
            "22.975885979689533\n",
            "23.054420581567012\n",
            "23.07328086043767\n",
            "22.963639356945475\n",
            "23.040831380979505\n",
            "22.90984069844572\n",
            "22.94431838217334\n",
            "22.774294458703697\n",
            "22.82072705301288\n",
            "23.11505814268957\n",
            "23.082369457680244\n",
            "23.01068802356527\n",
            "22.944026298217032\n",
            "23.04065667532936\n",
            "22.84880473378842\n",
            "22.897965262434504\n",
            "22.985555453094815\n",
            "23.034010987420682\n",
            "22.97215990130898\n",
            "23.00177483679336\n",
            "22.905882262274794\n",
            "23.06744524534283\n",
            "23.044953510470933\n",
            "22.905989296119436\n",
            "23.10095953926723\n",
            "22.905560333995517\n",
            "22.81114890363668\n",
            "22.956261916045875\n",
            "23.120155905243262\n",
            "23.070108746587646\n",
            "22.988371926919136\n",
            "22.98453054135973\n",
            "23.002352266209083\n",
            "23.162750302207197\n",
            "23.080822347042545\n",
            "23.192601062872107\n",
            "23.024311850212797\n",
            "23.014488097618173\n",
            "22.999431145777493\n",
            "22.919267794797925\n",
            "22.985925376647973\n",
            "23.058345048058193\n",
            "23.063919077229084\n",
            "23.135037224585137\n",
            "23.027589677928482\n",
            "22.997274839083065\n",
            "23.045123848249993\n",
            "22.90903332513502\n",
            "23.079992987973956\n",
            "22.938704221468193\n",
            "22.946803586357554\n",
            "22.971465165951535\n",
            "23.061765929255337\n",
            "23.024380010350615\n",
            "23.03167296936118\n",
            "23.04038053662324\n",
            "23.095615894688244\n",
            "22.939359656869375\n",
            "23.01551607290886\n",
            "22.957069482999866\n",
            "22.915028910907218\n",
            "23.133772057700156\n",
            "23.015837056930447\n",
            "23.0109809140124\n",
            "22.990763770381356\n",
            "22.948233853667013\n",
            "23.082601228971697\n",
            "23.033589872658546\n",
            "22.95278616398955\n",
            "22.977916250636177\n",
            "23.018180063083857\n",
            "23.00683748437472\n",
            "22.998239430321807\n",
            "22.97919032127999\n",
            "22.911816334765682\n",
            "22.916882181403274\n",
            "22.908754983149798\n",
            "22.926713557280532\n",
            "22.951260420516757\n",
            "22.997683412524527\n",
            "23.09164336458545\n",
            "23.046987943023698\n",
            "23.04521294312525\n",
            "23.068559687866365\n",
            "23.074010256721444\n",
            "22.817531688635796\n",
            "23.005474890742043\n",
            "23.069501167933602\n",
            "22.96209365104715\n",
            "22.96322084356071\n",
            "22.901929591218792\n",
            "22.86438917847186\n",
            "23.016713310523457\n",
            "22.987387322965922\n",
            "22.92477059695921\n",
            "22.96314053268957\n",
            "23.027664098012128\n",
            "22.957812482062614\n",
            "22.838295429046607\n",
            "23.076073044232327\n",
            "23.149445818892907\n",
            "22.840014650276963\n",
            "23.165027493588234\n",
            "22.9418378394835\n",
            "22.988588508424463\n",
            "22.961724847871054\n",
            "22.98831004876956\n",
            "23.046685025581578\n",
            "23.076439937791857\n",
            "22.877342260294277\n",
            "23.111958184502676\n",
            "22.890827436584054\n",
            "22.82323369770242\n",
            "23.045652736228593\n",
            "22.842384956150134\n",
            "22.880449082155106\n",
            "23.08556689449065\n",
            "22.897930660116515\n",
            "22.9878701378917\n",
            "23.095862499493272\n",
            "23.02027688959722\n",
            "22.940576811394912\n",
            "23.10330505242879\n",
            "22.866555503310718\n",
            "22.977129814452816\n",
            "23.063257573393734\n",
            "23.096121272617644\n",
            "23.06179412195028\n",
            "22.96028374472259\n",
            "22.92964323831414\n",
            "23.182626170791146\n",
            "22.995943923938672\n",
            "23.08423807038023\n",
            "23.04622770576846\n",
            "22.967803967812614\n",
            "23.056094635348046\n",
            "22.943036068494543\n",
            "22.824831647432774\n",
            "22.90172832474424\n",
            "23.027702925280124\n",
            "23.038242825984042\n",
            "23.054290075561614\n",
            "22.9656850563858\n",
            "22.948585205248104\n",
            "22.99494617644537\n",
            "22.976501678190736\n",
            "22.91525306053173\n",
            "22.9723564233993\n",
            "22.95978537225332\n",
            "22.957291832629483\n",
            "22.925343535079453\n",
            "22.88204244780515\n",
            "23.052109610253417\n",
            "23.084857392797737\n",
            "23.042975580038437\n",
            "22.997595630027707\n",
            "22.981739060370508\n",
            "23.001452052148355\n",
            "23.11438740271593\n",
            "22.986308247550642\n",
            "23.00824591174787\n",
            "22.796145494569977\n",
            "22.92693307084466\n",
            "23.144593687394625\n",
            "22.866808028808233\n",
            "23.079068181124086\n",
            "22.836760888709627\n",
            "22.99520528898291\n",
            "23.062506541837504\n",
            "23.03735812315697\n",
            "23.007795065596643\n",
            "22.99602285408299\n",
            "22.972325045030278\n",
            "22.961116396326137\n",
            "23.04654835797396\n",
            "22.905536503199993\n",
            "22.95072880538185\n",
            "23.016472088803226\n",
            "22.87225151818688\n",
            "23.13954640880625\n",
            "23.036834220045183\n",
            "22.982312634571297\n",
            "22.97786835317992\n",
            "22.88186500713479\n",
            "22.886286988747038\n",
            "22.964372300001127\n",
            "23.039180354567645\n",
            "22.971350009944928\n",
            "23.03224298735093\n",
            "23.06504442544181\n",
            "22.946258579032065\n",
            "23.003323786855095\n",
            "22.968862058504975\n",
            "22.95183897752438\n",
            "23.03234563609265\n",
            "23.05355954015317\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23.215121488189414, 0.11708163265306136)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss for the different learning rates used\n",
        "Utility.plot_results(lr_results[0][0], lr_results[1][0],\n",
        "                     lr_results[2][0], lr_results[3][0],\n",
        "                     labels=('lr=0.002', 'lr=0.001', 'lr=5e-4', 'lr=1e-4'),\n",
        "                     ylabel='Cross-entropy loss',\n",
        "                     title='Effect of learning rate on training loss')"
      ],
      "metadata": {
        "id": "jqvZ1K3zp_SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training accuracy for the different learning rates used\n",
        "Utility.plot_results(lr_results[0][1], lr_results[1][1],\n",
        "                     lr_results[2][1], lr_results[3][1],\n",
        "                     labels=('lr=0.002', 'lr=0.001', 'lr=5e-4', 'lr=1e-4'),\n",
        "                     ylabel='Accuracy', ymax=1.0,\n",
        "                     title='Effect of learning rate on training accuracy')"
      ],
      "metadata": {
        "id": "YhxS1S9PqsKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing the architecture\n",
        "\n",
        "*   Number of hidden layers\n",
        "*   Number of neurons per layer\n",
        "\n"
      ],
      "metadata": {
        "id": "pitXNowVqzBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP with 2 hidden layers\n",
        "p = Perceptron((width*width, 128, 64, 10),\n",
        "                  (Activation.relu, Activation.relu, Activation.softmax),\n",
        "                  Loss.cross_entropy, batch_size=100)\n",
        "\n",
        "# Run, store results in variable\n",
        "r2 = Utility.train_validate(p, mnist_data, lr=5e-4, epochs=25, verbose=True)\n",
        "\n",
        "# Plot results\n",
        "Utility.plot_results(r2[0], r2[1], r2[2], r2[3],\n",
        "                     labels=('Test loss', 'Test accuracy', 'Validation loss', \n",
        "                             'Validation accuracy'),\n",
        "                     fmts=('', '', 'o', 'o'), ymax=1.0,\n",
        "                     title='MLP with two hidden layers')"
      ],
      "metadata": {
        "id": "9YCf69NSnXPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP with 3 hidden layers\n",
        "p = Perceptron((width*width, 128, 64, 32, 10),\n",
        "               (Activation.relu, Activation.relu, Activation.relu, Activation.softmax),\n",
        "               Loss.cross_entropy, batch_size=100)\n",
        "r3 = Utility.train_validate(p, mnist_data, lr=5e-4, epochs=25, verbose=True)\n",
        "Utility.plot_results(r3[0], r3[1], r3[2], r3[3],\n",
        "                     labels=('Test loss', 'Test accuracy', 'Validation loss', \n",
        "                             'Validation accuracy'),\n",
        "                     fmts=('', '', 'o', 'o'), ymax=1.0,\n",
        "                     title='MLP with three hidden layers')"
      ],
      "metadata": {
        "id": "nUYB13p7pSC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP with 2 hidden layers, fewer neurons\n",
        "mlp = Perceptron((width*width, 16, 8, 10),\n",
        "                  (Activation.relu, Activation.relu, Activation.softmax),\n",
        "                  Loss.cross_entropy, batch_size=100)\n",
        "r2b = Utility.train_validate(mlp, mnist_data, lr=2e-3, epochs=25, verbose=True)\n",
        "Utility.plot_results(r2b[0], r2b[1], r2b[2], r2b[3],\n",
        "                     labels=('Test loss', 'Test accuracy', 'Validation loss', \n",
        "                             'Validation accuracy'),\n",
        "                     fmts=('', '', 'o', 'o'), ymax=1.0,\n",
        "                     title='MLP with two hidden layers, fewer neurons')"
      ],
      "metadata": {
        "id": "VPlcjxVgzj69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP with fewer neurons in hidden layers is more efficient to train but results in more loss and less accuracy. "
      ],
      "metadata": {
        "id": "mgMLR3t10yDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting activation functions"
      ],
      "metadata": {
        "id": "8LPlWqmM17Q4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing loss function"
      ],
      "metadata": {
        "id": "dnf7-wMhnl_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing how parameters are initialized"
      ],
      "metadata": {
        "id": "sZmoUz9un-2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Miscellaneous tasks\n",
        "\n",
        "Toy tests to manually check basic functionality"
      ],
      "metadata": {
        "id": "62TdC6i4TRvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hinge loss examples on 1/20/22 lecture:"
      ],
      "metadata": {
        "id": "FrMKJXoC_9wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-class classification example\n",
        "ex1 = np.array([-3.7, 5, 7]).T\n",
        "print('Target class is 3, loss={:0.1f} \\tif 2, {:0.1f} \\tif 1, {:0.1f}'.format(\n",
        "    Loss.hinge_loss(ex1, np.array([0, 0, 1]).T),\n",
        "    Loss.hinge_loss(ex1, np.array([0, 1, 0]).T),\n",
        "    Loss.hinge_loss(ex1, np.array([1, 0, 0]).T)))"
      ],
      "metadata": {
        "id": "nn7mgrr_ABYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7452b91-ea7d-4cb9-ec08-6b64e55c38dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target class is 3, loss=0.0 \tif 2, 3.0 \tif 1, 21.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-class classification example\n",
        "ex2 = np.array([2.5, 2.0, 2.7, 1.7]).T\n",
        "print('Target class is 2: {:0.1f}'.format(\n",
        "    Loss.hinge_loss(ex2, np.array([0, 1, 0, 0]).T)))"
      ],
      "metadata": {
        "id": "G3r5ejAXAEZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a4a381a-a8d4-4d84-fe24-90a788fec480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target class is 2: 3.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute accuracy on two examples, one correct and one not:"
      ],
      "metadata": {
        "id": "4jOLOwuQF6mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = np.array(((0.2, 0.5, 0.3), (0.7, 0.2, 0.1))).T\n",
        "y = np.array(((0, 1, 0), (0, 1, 0))).T\n",
        "Loss.accuracy(yhat, y)"
      ],
      "metadata": {
        "id": "EpN_cvn8F8_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b02d4761-73d4-4737-8c37-f93b674b7855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Informal problem given at the end of class, 1/25/22:"
      ],
      "metadata": {
        "id": "Fw_MM8svOBAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = Perceptron((4, 3, 2, 4), \n",
        "               (Activation.relu, Activation.sigmoid, Activation.softmax), \n",
        "               Loss.cross_entropy, init_with_normal=True)\n",
        "p[1].w = np.array([[1.,0,0,0], [0,1,0,0], [0,0,1,0]])\n",
        "p[2].w = np.array([[1.,0,0], [0,1,0]])\n",
        "p[3].w = np.array([[1.,0], [0,1], [0,0], [0,0]])\n",
        "p.zero_biases()\n",
        "\n",
        "# Check forward path\n",
        "p.forward(np.array([1, 0, 1, 0]).reshape(-1, 1), batch_size=1)\n",
        "for i in range(1, len(p.dims)):\n",
        "  print('Layer', i, '\\nz =\\n', p[i].z, '\\na =\\n', p[i].a, '\\n')"
      ],
      "metadata": {
        "id": "Z6ZL6w-XMa9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check backpropagation\n",
        "y = np.array([0., 1., 0., 0.]).reshape(-1, 1) # Dummy one-hot label\n",
        "print('p[1].w before backprop\\n', p[1].w)\n",
        "p.backward(y, lr=0.5, batch_size=1)\n",
        "print('p[1].w after backprop\\n', p[1].w)\n",
        "\n",
        "Activation.softmax(p[3].w @ p[2].a + p[3].b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDOfKgdXt6Gf",
        "outputId": "627d2d71-b63f-40dd-ea5f-0884326eadaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p[1].w before backprop\n",
            " [[1.04421637 0.         0.04421637 0.        ]\n",
            " [0.         1.         0.         0.        ]\n",
            " [0.03964693 0.         1.03964693 0.        ]]\n",
            "layer 1\n",
            "p[1].w after backprop\n",
            " [[1.14514486 0.         0.14514486 0.        ]\n",
            " [0.         1.         0.         0.        ]\n",
            " [0.09621592 0.         1.09621592 0.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.05991671],\n",
              "       [0.8446309 ],\n",
              "       [0.04772619],\n",
              "       [0.04772619]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    }
  ]
}